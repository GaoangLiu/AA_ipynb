{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis on Movie Reviews Naive Bayes.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOMGFW9bX8KBnhJi2WqXqJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHFtU0ZhbETM",
        "colab_type": "text"
      },
      "source": [
        "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n",
        "\n",
        "train.tsv contains the phrases and their associated sentiment \n",
        "\n",
        "labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n",
        "\n",
        "test.tsv contains just phrases. You must assign a sentiment label to each phrase.\n",
        "\n",
        "The sentiment labels are:\n",
        "\n",
        "0 - negative\n",
        "1 - somewhat negative\n",
        "2 - neutral\n",
        "3 - somewhat positive\n",
        "4 - positive\n",
        "\n",
        "Main page [https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=sampleSubmission.csv](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=sampleSubmission.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qxyqj5va8Zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import timeit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import logging\n",
        "import time\n",
        "import smart_open\n",
        "import importlib\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "logging.basicConfig(format='[%(asctime)s %(levelname)8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow_hub as tfh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwNz0M03bYlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.tsv *.zip\n",
        "! wget -O movie.zip ali.140714.xyz:8000/sentiment_analysis.zip \n",
        "! wget -O b7.py ali.140714.xyz:8000/boost117.py\n",
        "! unzip movie.zip \n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orj35i8ZbjSw",
        "colab_type": "text"
      },
      "source": [
        "# Explore data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cHL2NIEbklc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('max_colwidth', 300)\n",
        "train = pd.read_csv('train.tsv', sep=\"\\t\")\n",
        "train.columns = train.columns.str.lower()\n",
        "train.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuc7cbR2dpVq",
        "colab_type": "text"
      },
      "source": [
        "## Get hands \n",
        "\n",
        "NB score: 0.58055"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU3WlsNMdrLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    self.train = None\n",
        "    self.test = None \n",
        "    self.model = None\n",
        "    \n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv('train.tsv', sep=\"\\t\")\n",
        "      self.train.rename({'Phrase': 'text', 'Sentiment': 'target'}, axis='columns', inplace=True)\n",
        "      self.test = pd.read_csv('test.tsv', sep=\"\\t\")\n",
        "      self.test.rename({'Phrase': 'text', 'Sentiment': 'target'}, axis='columns', inplace=True)\n",
        "      logging.info('TSV data loaded')\n",
        "  \n",
        "  def countvectorize(self):\n",
        "    #   tv = TfidfVectorizer(ngram_range=(1,5), token_pattern=r'\\w{1,}',\n",
        "    #            min_df=2, max_df=0.9, strip_accents='unicode', \n",
        "    #            smooth_idf=1, sublinear_tf=1, max_features=5000)\n",
        "      tv = CountVectorizer()\n",
        "      tv.fit(pd.concat([self.train.text, self.test.text]))\n",
        "      self.vector_train = tv.transform(self.train.text)\n",
        "      self.vector_test  = tv.transform(self.test.text)\n",
        "      logging.info(\"Train & test text tokenized\")\n",
        "\n",
        "  def train_model(self):\n",
        "      # Choose your own classifier: self.model and run it\n",
        "      logging.info(f\"{self.__class__.__name__} starts running.\")\n",
        "      labels = self.train.target\n",
        "      X_train, X_val, y_train, y_val = train_test_split(self.vector_train, labels, test_size=0.2, random_state=2020)\n",
        "      self.model.fit(X_train, y_train)\n",
        "      \n",
        "      self.X_val, self.y_val = X_val, y_val \n",
        "      return self.model\n",
        "\n",
        "  def save_predictions(self, y_preds):\n",
        "      sub = pd.read_csv(f\"sampleSubmission.csv\")\n",
        "      sub['Sentiment'] = y_preds \n",
        "      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n",
        "      logging.info(f'Prediction exported to submission_{self.__class__.__name__}.csv')\n",
        "  \n",
        "  def pipeline(self):\n",
        "      s_time = time.clock()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "      self.build_model()\n",
        "      self.train_model()\n",
        "      logging.info(f\"Program running for {time.clock() - s_time} seconds\")\n",
        "\n",
        "class C_Bayes(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = MultinomialNB()\n",
        "      return self.model\n",
        "\n",
        "class C_SVM(Classifier):\n",
        "    def build_model(self):\n",
        "        self.model = svm.SVC(n_jobs=-1, verbose=True)\n",
        "        return self.model\n",
        "\n",
        "class C_LR(Classifier):\n",
        "    def build_model(self):\n",
        "        self.model = LogisticRegression(n_jobs=10, verbose=1, solver='lbfgs')\n",
        "        return self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlx78gOrB9N",
        "colab_type": "text"
      },
      "source": [
        "Try SVM\n",
        "Results, svm runs too slow. Quit "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AvRgz3orDeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfrom(text_train, text_test):\n",
        "    large_use = 'https://tfhub.dev/google/universal-sentence-encoder-large/5'\n",
        "    embed = tfh.load(large_use)\n",
        "\n",
        "    vector_train = [tf.reshape(embed([line]), [-1]).numpy() for line in tqdm(text_train)]\n",
        "    vector_test = [tf.reshape(embed([line]), [-1]).numpy() for line in tqdm(text_test)]\n",
        "\n",
        "    return vector_train, vector_test\n",
        "\n",
        "# base = Classifier()\n",
        "# base.load_data()\n",
        "# vector_train, vector_test = transfrom(base.train.text, base.test.text)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNhljX3Nkkd",
        "colab_type": "text"
      },
      "source": [
        "Try LR\n",
        "- first run produces score 0.61260, not bad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvTMtw8QrrJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = C_LR()\n",
        "c.pipeline()\n",
        "\n",
        "y_preds = c.model.predict(c.X_val)\n",
        "print('Accuracy score', accuracy_score(y_preds, c.y_val))\n",
        "\n",
        "y_preds = c.model.predict(c.vector_test)\n",
        "c.save_predictions(y_preds)\n",
        "y_preds \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnW0VwqKrgb-",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm7qWDJ5fDwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = C_Bayes()\n",
        "c.pipeline()\n",
        "y_preds = c.model.predict(c.vector_test)\n",
        "c.save_predictions(y_preds)\n",
        "y_preds\n",
        "\n",
        "# results 0.58055"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkMzAgrdgD3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls\n",
        "import b7\n",
        "b7.Files().upload_vps('submission_C_LR.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}