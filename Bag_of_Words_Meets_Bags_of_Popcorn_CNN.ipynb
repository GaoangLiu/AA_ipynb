{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag of Words Meets Bags of Popcorn - CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFQeyYx4IlST6KeA4+zKIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/AA_ipynb/blob/master/Bag_of_Words_Meets_Bags_of_Popcorn_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHIyU9uu4TP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "c2f60db4-6286-48a3-b68c-d5016a921e3f"
      },
      "source": [
        "!curl -o train.tsv ali.140714.xyz:8000/popcorn.tsv\n",
        "!curl -o test.tsv ali.140714.xyz:8000/popcorn_test.tsv\n",
        "# !curl -o master.csv ali.140714.xyz:8000/imdb_master.csv\n",
        "!curl -o sample.csv ali.140714.xyz:8000/popcorn_sample.csv"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 32.0M  100 32.0M    0     0  1266k      0  0:00:25  0:00:25 --:--:-- 1046k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31.2M  100 31.2M    0     0  1839k      0  0:00:17  0:00:17 --:--:-- 1335k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  129M  100  129M    0     0   861k      0  0:02:33  0:02:33 --:--:--  892k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  276k  100  276k    0     0   173k      0  0:00:01  0:00:01 --:--:--  173k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhYYxq91VcVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "dd8c0dc8-497f-4cdf-c97d-dd9c155b9c5e"
      },
      "source": [
        "extra = pd.read_csv('master.csv', encoding=\"latin-1\")\n",
        "extra.drop(extra[extra.label=='unsup'].index, inplace=True)\n",
        "extra['sentiment'] = (extra['label'] == 'pos').astype(int)\n",
        "# extra= extra[['review', 'sentiment']]\n",
        "# extra.label.value_counts(), extra\n",
        "extra\n",
        "\n",
        "# train = pd.read_csv('train.tsv', sep='\\t')\n",
        "# train = train[['review', 'sentiment']]\n",
        "# train = pd.concat([extra, train])\n",
        "# train.sentiment.value_counts()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>49995</td>\n",
              "      <td>train</td>\n",
              "      <td>Seeing as the vote average was pretty low, and...</td>\n",
              "      <td>pos</td>\n",
              "      <td>9998_9.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>49996</td>\n",
              "      <td>train</td>\n",
              "      <td>The plot had some wretched, unbelievable twist...</td>\n",
              "      <td>pos</td>\n",
              "      <td>9999_8.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>49997</td>\n",
              "      <td>train</td>\n",
              "      <td>I am amazed at how this movie(and most others ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>999_10.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>49998</td>\n",
              "      <td>train</td>\n",
              "      <td>A Christmas Together actually came before my t...</td>\n",
              "      <td>pos</td>\n",
              "      <td>99_8.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>49999</td>\n",
              "      <td>train</td>\n",
              "      <td>Working-class romantic drama from director Mar...</td>\n",
              "      <td>pos</td>\n",
              "      <td>9_7.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0   type  ...         file sentiment\n",
              "0               0   test  ...      0_2.txt         0\n",
              "1               1   test  ...  10000_4.txt         0\n",
              "2               2   test  ...  10001_1.txt         0\n",
              "3               3   test  ...  10002_3.txt         0\n",
              "4               4   test  ...  10003_3.txt         0\n",
              "...           ...    ...  ...          ...       ...\n",
              "49995       49995  train  ...   9998_9.txt         1\n",
              "49996       49996  train  ...   9999_8.txt         1\n",
              "49997       49997  train  ...   999_10.txt         1\n",
              "49998       49998  train  ...     99_8.txt         1\n",
              "49999       49999  train  ...      9_7.txt         1\n",
              "\n",
              "[50000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh9DANZL4ulk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import sklearn.metrics\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbdb0Wd-7T0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7879c7f0-52e5-463b-9e73-640ed5ae0303"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import sklearn\n",
        "import sklearn.metrics \n",
        "import sklearn.model_selection\n",
        "import sklearn.feature_extraction\n",
        "from absl import logging\n",
        "import gensim.downloader as api\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "class OverwriteLog():\n",
        "    # overwrite logging for kaggle kernel\n",
        "    def info(self, msg):\n",
        "        print(msg)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token, 'v') for token in text.split(\" \")]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "def load_data():\n",
        "    train = pd.read_csv('train.tsv', sep='\\t')\n",
        "    test = pd.read_csv('test.tsv', sep='\\t')\n",
        "\n",
        "    # extra = pd.read_csv('master.csv', encoding=\"latin-1\")\n",
        "    # extra.drop(extra[extra.label=='unsup'].index, inplace=True)\n",
        "    # extra['sentiment'] = (extra['label'] == 'pos').astype(int)\n",
        "    # extra= extra[['review', 'sentiment']]\n",
        "\n",
        "    # train = train[['review', 'sentiment']]\n",
        "    # train = pd.concat([extra, train])\n",
        "\n",
        "    train['text'] = train.review.apply(clean_text)\n",
        "    train['target'] = train.sentiment\n",
        "    test['text'] = test.review\n",
        "    return train, test\n",
        "    \n",
        "\n",
        "def countvectorize(train, test, vocab_size, sentence_len):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "    text = pd.concat([train.text, test.text])\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    sequence_train = tokenizer.texts_to_sequences(train.text)\n",
        "    train_inputs = tf.keras.preprocessing.sequence.pad_sequences(sequence_train, maxlen=sentence_len)\n",
        "    logging.info('Train text tokeninzed')\n",
        "\n",
        "    sequence_test = tokenizer.texts_to_sequences(test.text)\n",
        "    test_inputs = tf.keras.preprocessing.sequence.pad_sequences(sequence_test, maxlen=sentence_len)\n",
        "    logging.info('Test text tokeninzed')\n",
        "\n",
        "    return train_inputs, test_inputs, tokenizer\n",
        "\n",
        "def embed_word_vector(word_index, vocab_size=10000, embed_size=128, model='glove-wiki-gigaword-100'):\n",
        "    glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "    zeros = [0] * embed_size\n",
        "    matrix = np.zeros((vocab_size, embed_size))\n",
        "      \n",
        "    for word, i in word_index.items(): \n",
        "        if i >= vocab_size or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "        matrix[i] = glove[word]\n",
        "\n",
        "    logging.info('Matrix with embedded word vector created')\n",
        "    return matrix\n",
        "\n",
        "def build_model(vocab_size = 10000, max_len = 100, embed_size = 128, embed_matrix=[]):\n",
        "    text_input = tf.keras.Input(shape=(max_len, ))\n",
        "    embed_text = tf.keras.layers.Embedding(vocab_size, embed_size)(text_input)\n",
        "    if len(embed_matrix) > 0:\n",
        "        embed_text = tf.keras.layers.Embedding(vocab_size, embed_size, \\\n",
        "                                        weights=[embed_matrix], trainable=False)(text_input)\n",
        "        \n",
        "    net = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embed_text)\n",
        "    net = tf.keras.layers.GlobalMaxPool1D()(net)\n",
        "\n",
        "    net = tf.keras.layers.Dense(64, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "\n",
        "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=text_input, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\"\"\"split the following codes into several chunks in Jyputer \n",
        "for clearer reading and saved variables\n",
        "\"\"\""
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'split the following codes into several chunks in Jyputer \\nfor clearer reading and saved variables\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJVklM5w7WF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c382aa99-246d-46ed-e8fd-fda5ace1d280"
      },
      "source": [
        "vocab_size = 30_000\n",
        "sentence_len = 250\n",
        "embed_size = 300\n",
        "\n",
        "train, test = load_data()\n",
        "train_inputs, test_inputs, tokenizer = countvectorize(train, test, vocab_size, sentence_len)\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train_inputs, train.target, test_size=0.2, random_state=0)\n",
        "# matrix = embed_word_vector(tokenizer.word_index, vocab_size, embed_size, \"glove-wiki-gigaword-300\")\n",
        "matrix = embed_word_vector(tokenizer.word_index, vocab_size, embed_size, \"fasttext-wiki-news-subwords-300\")\n",
        "logging.info(\"Data loaded and split\")\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Train text tokeninzed\n",
            "INFO:absl:Test text tokeninzed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "INFO:absl:Matrix with embedded word vector created\n",
            "INFO:absl:Data loaded and split\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibNkG4wiC1ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_inputs.shape\n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "# text = pd.concat([train.text, test.text])\n",
        "# tokenizer.fit_on_texts(text)\n",
        "# tokenizer.num_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGVz8t7ebPz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrAvo-Hd7YQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "f1335dd0-0c65-4c44-d719-6dde885fdd3f"
      },
      "source": [
        "# Build model \n",
        "model = build_model(vocab_size, sentence_len, embed_size, matrix)\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "logging.info(\"Model built\")\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Model built\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 250)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_11 (Embedding)     (None, 250, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 250, 256)          439296    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_5 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 9,457,857\n",
            "Trainable params: 457,857\n",
            "Non-trainable params: 9,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHZIWz5B7aFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31e44719-297d-4f07-e315-6bd6cb69ef44"
      },
      "source": [
        "# Run model\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('best.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=1024,\n",
        "    verbose=1\n",
        ")\n",
        "logging.info(\"Model trainning complete\")\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.5475\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.64120, saving model to best.h5\n",
            "20/20 [==============================] - 6s 310ms/step - loss: 0.6873 - accuracy: 0.5475 - val_loss: 0.6637 - val_accuracy: 0.6412\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.6466\n",
            "Epoch 00002: val_accuracy improved from 0.64120 to 0.72240, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.6430 - accuracy: 0.6466 - val_loss: 0.6038 - val_accuracy: 0.7224\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.5464 - accuracy: 0.7409\n",
            "Epoch 00003: val_accuracy improved from 0.72240 to 0.80420, saving model to best.h5\n",
            "20/20 [==============================] - 6s 283ms/step - loss: 0.5464 - accuracy: 0.7409 - val_loss: 0.4505 - val_accuracy: 0.8042\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.7811\n",
            "Epoch 00004: val_accuracy improved from 0.80420 to 0.81300, saving model to best.h5\n",
            "20/20 [==============================] - 6s 281ms/step - loss: 0.4817 - accuracy: 0.7811 - val_loss: 0.4246 - val_accuracy: 0.8130\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8162\n",
            "Epoch 00005: val_accuracy improved from 0.81300 to 0.82260, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.4207 - accuracy: 0.8162 - val_loss: 0.4024 - val_accuracy: 0.8226\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.8141\n",
            "Epoch 00006: val_accuracy improved from 0.82260 to 0.84140, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.4185 - accuracy: 0.8141 - val_loss: 0.3744 - val_accuracy: 0.8414\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.8312\n",
            "Epoch 00007: val_accuracy improved from 0.84140 to 0.84640, saving model to best.h5\n",
            "20/20 [==============================] - 6s 281ms/step - loss: 0.3917 - accuracy: 0.8312 - val_loss: 0.3592 - val_accuracy: 0.8464\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.8302\n",
            "Epoch 00008: val_accuracy improved from 0.84640 to 0.84940, saving model to best.h5\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.3939 - accuracy: 0.8302 - val_loss: 0.3591 - val_accuracy: 0.8494\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.8347\n",
            "Epoch 00009: val_accuracy did not improve from 0.84940\n",
            "20/20 [==============================] - 5s 274ms/step - loss: 0.3925 - accuracy: 0.8347 - val_loss: 0.3668 - val_accuracy: 0.8454\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3768 - accuracy: 0.8359\n",
            "Epoch 00010: val_accuracy improved from 0.84940 to 0.84960, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.3768 - accuracy: 0.8359 - val_loss: 0.3598 - val_accuracy: 0.8496\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8508\n",
            "Epoch 00011: val_accuracy improved from 0.84960 to 0.86260, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.3548 - accuracy: 0.8508 - val_loss: 0.3335 - val_accuracy: 0.8626\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.8605\n",
            "Epoch 00012: val_accuracy improved from 0.86260 to 0.86540, saving model to best.h5\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.3368 - accuracy: 0.8605 - val_loss: 0.3208 - val_accuracy: 0.8654\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3350 - accuracy: 0.8595\n",
            "Epoch 00013: val_accuracy improved from 0.86540 to 0.86620, saving model to best.h5\n",
            "20/20 [==============================] - 6s 283ms/step - loss: 0.3350 - accuracy: 0.8595 - val_loss: 0.3159 - val_accuracy: 0.8662\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.8672\n",
            "Epoch 00014: val_accuracy improved from 0.86620 to 0.86840, saving model to best.h5\n",
            "20/20 [==============================] - 6s 285ms/step - loss: 0.3202 - accuracy: 0.8672 - val_loss: 0.3092 - val_accuracy: 0.8684\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.8732\n",
            "Epoch 00015: val_accuracy improved from 0.86840 to 0.87260, saving model to best.h5\n",
            "20/20 [==============================] - 6s 282ms/step - loss: 0.3060 - accuracy: 0.8732 - val_loss: 0.3065 - val_accuracy: 0.8726\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8768\n",
            "Epoch 00016: val_accuracy did not improve from 0.87260\n",
            "20/20 [==============================] - 6s 277ms/step - loss: 0.3002 - accuracy: 0.8768 - val_loss: 0.3011 - val_accuracy: 0.8726\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.8830\n",
            "Epoch 00017: val_accuracy did not improve from 0.87260\n",
            "20/20 [==============================] - 5s 272ms/step - loss: 0.2865 - accuracy: 0.8830 - val_loss: 0.3104 - val_accuracy: 0.8628\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.8817\n",
            "Epoch 00018: val_accuracy improved from 0.87260 to 0.87280, saving model to best.h5\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.2869 - accuracy: 0.8817 - val_loss: 0.3021 - val_accuracy: 0.8728\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.8853\n",
            "Epoch 00019: val_accuracy improved from 0.87280 to 0.87940, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.2795 - accuracy: 0.8853 - val_loss: 0.2901 - val_accuracy: 0.8794\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.8929\n",
            "Epoch 00020: val_accuracy did not improve from 0.87940\n",
            "20/20 [==============================] - 5s 273ms/step - loss: 0.2655 - accuracy: 0.8929 - val_loss: 0.2866 - val_accuracy: 0.8782\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.8988\n",
            "Epoch 00021: val_accuracy did not improve from 0.87940\n",
            "20/20 [==============================] - 5s 273ms/step - loss: 0.2558 - accuracy: 0.8988 - val_loss: 0.2844 - val_accuracy: 0.8786\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9044\n",
            "Epoch 00022: val_accuracy improved from 0.87940 to 0.88240, saving model to best.h5\n",
            "20/20 [==============================] - 6s 280ms/step - loss: 0.2430 - accuracy: 0.9044 - val_loss: 0.2906 - val_accuracy: 0.8824\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.9076\n",
            "Epoch 00023: val_accuracy did not improve from 0.88240\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.2374 - accuracy: 0.9076 - val_loss: 0.2977 - val_accuracy: 0.8780\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9111\n",
            "Epoch 00024: val_accuracy did not improve from 0.88240\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.2282 - accuracy: 0.9111 - val_loss: 0.2951 - val_accuracy: 0.8806\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9171\n",
            "Epoch 00025: val_accuracy did not improve from 0.88240\n",
            "20/20 [==============================] - 6s 278ms/step - loss: 0.2135 - accuracy: 0.9171 - val_loss: 0.2851 - val_accuracy: 0.8790\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9244\n",
            "Epoch 00026: val_accuracy did not improve from 0.88240\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.2041 - accuracy: 0.9244 - val_loss: 0.3009 - val_accuracy: 0.8708\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9314\n",
            "Epoch 00027: val_accuracy did not improve from 0.88240\n",
            "20/20 [==============================] - 6s 279ms/step - loss: 0.1898 - accuracy: 0.9314 - val_loss: 0.2907 - val_accuracy: 0.8808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Model trainning complete\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 00027: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMDZPqIr7bZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7741e40c-7147-4258-9525-66f769ca182b"
      },
      "source": [
        "# validation & predict\n",
        "model.load_weights('best.h5')\n",
        "y_preds = model.predict(test_inputs)\n",
        "logging.info(\"Prediction DONE\")\n",
        "\n",
        "\n",
        "sub = pd.read_csv('sample.csv')\n",
        "sub['sentiment'] = y_preds\n",
        "sub.to_csv('popcorn_submission.csv', index=False)\n",
        "!curl -X PUT --upload-file popcorn_submission.csv ali.140714.xyz:8000/\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Prediction DONE\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            " 97  443k    0     0   97  432k      0  98764  0:00:04  0:00:04 --:--:-- 98742\n",
            "\n",
            "## Received: \"popcorn_submission.csv\"\n",
            "\n",
            "100  443k    0    41  100  443k      8  90540  0:00:05  0:00:05 --:--:-- 99580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KSZ930wpc0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub = pd.read_csv('sample.csv')\n",
        "sub['sentiment'] = y_preds\n",
        "# sub.to_csv('popcorn_submission.csv', index=False)\n",
        "sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Db-E74zskB",
        "colab_type": "text"
      },
      "source": [
        "# Bert imbedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6UzSf9zzwAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "3cd3ce42-5afd-430a-ea9d-dba4fe0982de"
      },
      "source": [
        "train, test = load_data()\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train['text'], train['target'], random_state=0)\n",
        "\n",
        "model = 'https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1'\n",
        "model = 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'\n",
        "hub_layer = hub.KerasLayer(model, output_shape=[128], input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "\n",
        "input = tf.keras.Input(shape=(), name=\"Input\", dtype=tf.string)\n",
        "net = hub_layer(input)\n",
        "net = tf.keras.layers.Dense(64)(net)\n",
        "net = tf.keras.layers.Dense(32)(net)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
        "model = tf.keras.models.Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           [(None,)]                 0         \n",
            "_________________________________________________________________\n",
            "keras_layer_6 (KerasLayer)   (None, 128)               124642688 \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 124,653,057\n",
            "Trainable params: 124,653,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_uKd4w0qkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss = tf.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer = 'adam',\n",
        "    metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
        "\n",
        "cp = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='best_weights.hdf5', monitor='val_accuracy', \n",
        "    verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Early stopping \n",
        "es = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto',\n",
        "    baseline=None, restore_best_weights=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRGjreEJ0-HE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "acb2adf1-2f59-4d64-b71b-9f0f2623ed52"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "          validation_data=(X_val, y_val),\n",
        "          batch_size=128,\n",
        "          callbacks=[cp, es],\n",
        "          epochs = 30, verbose=1)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "147/147 [==============================] - ETA: 0s - loss: 0.6044 - accuracy: 0.7651\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88608, saving model to best_weights.hdf5\n",
            "147/147 [==============================] - 14s 92ms/step - loss: 0.6044 - accuracy: 0.7651 - val_loss: 0.5622 - val_accuracy: 0.8861\n",
            "Epoch 2/30\n",
            "147/147 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.9277\n",
            "Epoch 00002: val_accuracy improved from 0.88608 to 0.88944, saving model to best_weights.hdf5\n",
            "147/147 [==============================] - 40s 273ms/step - loss: 0.5361 - accuracy: 0.9277 - val_loss: 0.5585 - val_accuracy: 0.8894\n",
            "Epoch 3/30\n",
            "146/147 [============================>.] - ETA: 0s - loss: 0.5209 - accuracy: 0.9616\n",
            "Epoch 00003: val_accuracy did not improve from 0.88944\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.5208 - accuracy: 0.9615 - val_loss: 0.5565 - val_accuracy: 0.8880\n",
            "Epoch 4/30\n",
            "147/147 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.9701\n",
            "Epoch 00004: val_accuracy did not improve from 0.88944\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.5164 - accuracy: 0.9701 - val_loss: 0.5588 - val_accuracy: 0.8853\n",
            "Epoch 5/30\n",
            "147/147 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.9766\n",
            "Epoch 00005: val_accuracy did not improve from 0.88944\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.5134 - accuracy: 0.9766 - val_loss: 0.5598 - val_accuracy: 0.8880\n",
            "Epoch 6/30\n",
            "147/147 [==============================] - ETA: 0s - loss: 0.5126 - accuracy: 0.9783\n",
            "Epoch 00006: val_accuracy did not improve from 0.88944\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.5126 - accuracy: 0.9783 - val_loss: 0.5627 - val_accuracy: 0.8851\n",
            "Epoch 7/30\n",
            "146/147 [============================>.] - ETA: 0s - loss: 0.5113 - accuracy: 0.9811\n",
            "Epoch 00007: val_accuracy did not improve from 0.88944\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.5114 - accuracy: 0.9811 - val_loss: 0.5626 - val_accuracy: 0.8858\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FNMWHOy1MQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5a79cbe6-0fd3-4eb0-91ce-369e45864135"
      },
      "source": [
        "model.load_weights('best_weights.hdf5')\n",
        "y_preds = model.predict(test.text)\n",
        "logging.info(\"Prediction DONE\")\n",
        "\n",
        "\n",
        "sub = pd.read_csv('sample.csv')\n",
        "sub['sentiment'] = y_preds\n",
        "sub.to_csv('popcorn_submission.csv', index=False)\n",
        "!curl -X PUT --upload-file popcorn_submission.csv ali.140714.xyz:8000/"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Prediction DONE\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  453k    0     0  100  453k      0  91975  0:00:05  0:00:05 --:--:-- 92249\n",
            "\n",
            "## Received: \"popcorn_submission.csv\"\n",
            "\n",
            "100  453k    0    41  100  453k      8  91975  0:00:05  0:00:05 --:--:--  128k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7TsjT9l2PJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "568e48ca-c481-4223-d29d-6c73752393c4"
      },
      "source": [
        "y_preds"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0000000e+00],\n",
              "       [9.0371032e-04],\n",
              "       [5.4717118e-01],\n",
              "       ...,\n",
              "       [1.4462286e-03],\n",
              "       [9.9999976e-01],\n",
              "       [9.3779229e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}