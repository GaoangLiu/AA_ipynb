{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag of Words Meets Bags of Popcorn - CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/F2pa9ycVfFYpfwkQC7nM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/AA_ipynb/blob/master/Bag_of_Words_Meets_Bags_of_Popcorn_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHIyU9uu4TP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "8d588e62-f618-424b-8c68-f36931757e45"
      },
      "source": [
        "!curl -o train.tsv ali.140714.xyz:8000/popcorn.tsv\n",
        "!curl -o test.tsv ali.140714.xyz:8000/popcorn_test.tsv\n",
        "!curl -o master.csv ali.140714.xyz:8000/imdb_master.csv\n",
        "!curl -o sample.csv ali.140714.xyz:8000/popcorn_sample.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 32.0M  100 32.0M    0     0  3427k      0  0:00:09  0:00:09 --:--:-- 4120k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31.2M  100 31.2M    0     0  3271k      0  0:00:09  0:00:09 --:--:-- 3919k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  129M  100  129M    0     0  3779k      0  0:00:35  0:00:35 --:--:-- 4094k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  276k  100  276k    0     0   171k      0  0:00:01  0:00:01 --:--:--  171k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhYYxq91VcVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "db17b424-a84a-4632-9d6e-5a20e079b8f2"
      },
      "source": [
        "extra = pd.read_csv('master.csv', encoding=\"latin-1\")\n",
        "extra.drop(extra[extra.label=='unsup'].index, inplace=True)\n",
        "extra['sentiment'] = (extra['label'] == 'pos').astype(int)\n",
        "extra= extra[['review', 'sentiment']]\n",
        "# extra.label.value_counts(), extra\n",
        "extra\n",
        "\n",
        "train = pd.read_csv('train.tsv', sep='\\t')\n",
        "train = train[['review', 'sentiment']]\n",
        "train = pd.concat([extra, train])\n",
        "train.sentiment.value_counts()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    37500\n",
              "0    37500\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh9DANZL4ulk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "643a08d1-0c47-4f03-e54e-c7a2c46f6dc8"
      },
      "source": [
        "import tensorflow as tf \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import sklearn.metrics\n",
        "import seaborn as sns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbdb0Wd-7T0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b79206b6-8d2f-493e-c9c0-9c320215f6f4"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import sklearn\n",
        "import sklearn.metrics \n",
        "import sklearn.model_selection\n",
        "import sklearn.feature_extraction\n",
        "from absl import logging\n",
        "import gensim.downloader as api\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "class OverwriteLog():\n",
        "    # overwrite logging for kaggle kernel\n",
        "    def info(self, msg):\n",
        "        print(msg)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token, 'v') for token in text.split(\" \")]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "def load_data():\n",
        "    train = pd.read_csv('train.tsv', sep='\\t')\n",
        "    test = pd.read_csv('test.tsv', sep='\\t')\n",
        "\n",
        "    extra = pd.read_csv('master.csv', encoding=\"latin-1\")\n",
        "    extra.drop(extra[extra.label=='unsup'].index, inplace=True)\n",
        "    extra['sentiment'] = (extra['label'] == 'pos').astype(int)\n",
        "    extra= extra[['review', 'sentiment']]\n",
        "\n",
        "    train = train[['review', 'sentiment']]\n",
        "    train = pd.concat([extra, train])\n",
        "\n",
        "    train['text'] = train.review\n",
        "    train['target'] = train.sentiment\n",
        "    test['text'] = test.review\n",
        "    return train, test\n",
        "    \n",
        "\n",
        "def countvectorize(train, test, vocab_size, sentence_len):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "    text = pd.concat([train.text, test.text])\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    sequence_train = tokenizer.texts_to_sequences(train.text)\n",
        "    train_inputs = tf.keras.preprocessing.sequence.pad_sequences(sequence_train, maxlen=sentence_len)\n",
        "    logging.info('Train text tokeninzed')\n",
        "\n",
        "    sequence_test = tokenizer.texts_to_sequences(test.text)\n",
        "    test_inputs = tf.keras.preprocessing.sequence.pad_sequences(sequence_test, maxlen=sentence_len)\n",
        "    logging.info('Test text tokeninzed')\n",
        "\n",
        "    return train_inputs, test_inputs, tokenizer\n",
        "\n",
        "def embed_word_vector(word_index, vocab_size=10000, embed_size=128, model='glove-wiki-gigaword-100'):\n",
        "    glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "    zeros = [0] * embed_size\n",
        "    matrix = np.zeros((vocab_size, embed_size))\n",
        "      \n",
        "    for word, i in word_index.items(): \n",
        "        if i >= vocab_size or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "        matrix[i] = glove[word]\n",
        "\n",
        "    logging.info('Matrix with embedded word vector created')\n",
        "    return matrix\n",
        "\n",
        "def build_model(vocab_size = 10000, max_len = 100, embed_size = 128, embed_matrix=[]):\n",
        "    text_input = tf.keras.Input(shape=(max_len, ))\n",
        "    embed_text = tf.keras.layers.Embedding(vocab_size, embed_size)(text_input)\n",
        "    if len(embed_matrix) > 0:\n",
        "        embed_text = tf.keras.layers.Embedding(vocab_size, embed_size, \\\n",
        "                                        weights=[embed_matrix], trainable=False)(text_input)\n",
        "        \n",
        "    net = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embed_text)\n",
        "    net = tf.keras.layers.GlobalMaxPool1D()(net)\n",
        "\n",
        "    net = tf.keras.layers.Dense(64, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "\n",
        "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=text_input, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\"\"\"split the following codes into several chunks in Jyputer \n",
        "for clearer reading and saved variables\n",
        "\"\"\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'split the following codes into several chunks in Jyputer \\nfor clearer reading and saved variables\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJVklM5w7WF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7ba4fa94-3f4f-4724-bea8-c30a1418b2db"
      },
      "source": [
        "vocab_size = 10000\n",
        "sentence_len = 150\n",
        "embed_size = 300\n",
        "\n",
        "train, test = load_data()\n",
        "train_inputs, test_inputs, tokenizer = countvectorize(train, test, vocab_size, sentence_len)\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train_inputs, train.target, test_size=0.2, random_state=0)\n",
        "matrix = embed_word_vector(tokenizer.word_index, vocab_size, embed_size, \"word2vec-google-news-300\")\n",
        "# matrix = embed_word_vector(tokenizer.word_index, vocab_size, embed_size, \"fasttext-wiki-news-subwords-300\")\n",
        "logging.info(\"Data loaded and split\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Train text tokeninzed\n",
            "INFO:absl:Test text tokeninzed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[===-----------------------------------------------] 7.3% 121.7/1662.8MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibNkG4wiC1ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_inputs.shape\n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "# text = pd.concat([train.text, test.text])\n",
        "# tokenizer.fit_on_texts(text)\n",
        "# tokenizer.num_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGVz8t7ebPz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrAvo-Hd7YQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "529b8c4b-8b2f-41e4-b784-85f5d59d8d00"
      },
      "source": [
        "# Build model \n",
        "model = build_model(vocab_size, sentence_len, embed_size, matrix)\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "logging.info(\"Model built\")\n",
        "\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Model built\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_21 (Embedding)     (None, 150, 300)          3000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, 150, 256)          439296    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_13 (Glo (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 3,457,857\n",
            "Trainable params: 457,857\n",
            "Non-trainable params: 3,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHZIWz5B7aFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3c19b7e-1f94-475b-a8ed-9f11c1cfb3db"
      },
      "source": [
        "# Run model\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('best.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=1024,\n",
        "    verbose=1\n",
        ")\n",
        "logging.info(\"Model trainning complete\")\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.6524\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.77920, saving model to best.h5\n",
            "20/20 [==============================] - 9s 464ms/step - loss: 0.6459 - accuracy: 0.6524 - val_loss: 0.5094 - val_accuracy: 0.7792\n",
            "Epoch 2/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.7879\n",
            "Epoch 00002: val_accuracy improved from 0.77920 to 0.82800, saving model to best.h5\n",
            "20/20 [==============================] - 8s 420ms/step - loss: 0.4693 - accuracy: 0.7879 - val_loss: 0.3969 - val_accuracy: 0.8280\n",
            "Epoch 3/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8206\n",
            "Epoch 00003: val_accuracy improved from 0.82800 to 0.84680, saving model to best.h5\n",
            "20/20 [==============================] - 8s 421ms/step - loss: 0.4004 - accuracy: 0.8206 - val_loss: 0.3555 - val_accuracy: 0.8468\n",
            "Epoch 4/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8416\n",
            "Epoch 00004: val_accuracy improved from 0.84680 to 0.85820, saving model to best.h5\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.3613 - accuracy: 0.8416 - val_loss: 0.3362 - val_accuracy: 0.8582\n",
            "Epoch 5/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8543\n",
            "Epoch 00005: val_accuracy improved from 0.85820 to 0.86340, saving model to best.h5\n",
            "20/20 [==============================] - 8s 422ms/step - loss: 0.3389 - accuracy: 0.8543 - val_loss: 0.3235 - val_accuracy: 0.8634\n",
            "Epoch 6/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8625\n",
            "Epoch 00006: val_accuracy did not improve from 0.86340\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.3236 - accuracy: 0.8625 - val_loss: 0.3528 - val_accuracy: 0.8472\n",
            "Epoch 7/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.8679\n",
            "Epoch 00007: val_accuracy improved from 0.86340 to 0.86400, saving model to best.h5\n",
            "20/20 [==============================] - 8s 421ms/step - loss: 0.3152 - accuracy: 0.8679 - val_loss: 0.3201 - val_accuracy: 0.8640\n",
            "Epoch 8/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.8788\n",
            "Epoch 00008: val_accuracy improved from 0.86400 to 0.87060, saving model to best.h5\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.2906 - accuracy: 0.8788 - val_loss: 0.3082 - val_accuracy: 0.8706\n",
            "Epoch 9/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8888\n",
            "Epoch 00009: val_accuracy improved from 0.87060 to 0.87400, saving model to best.h5\n",
            "20/20 [==============================] - 8s 422ms/step - loss: 0.2736 - accuracy: 0.8888 - val_loss: 0.3000 - val_accuracy: 0.8740\n",
            "Epoch 10/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.8913\n",
            "Epoch 00010: val_accuracy did not improve from 0.87400\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.2631 - accuracy: 0.8913 - val_loss: 0.3032 - val_accuracy: 0.8736\n",
            "Epoch 11/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9059\n",
            "Epoch 00011: val_accuracy improved from 0.87400 to 0.87940, saving model to best.h5\n",
            "20/20 [==============================] - 8s 422ms/step - loss: 0.2399 - accuracy: 0.9059 - val_loss: 0.2907 - val_accuracy: 0.8794\n",
            "Epoch 12/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9173\n",
            "Epoch 00012: val_accuracy improved from 0.87940 to 0.88260, saving model to best.h5\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.2160 - accuracy: 0.9173 - val_loss: 0.2915 - val_accuracy: 0.8826\n",
            "Epoch 13/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9251\n",
            "Epoch 00013: val_accuracy improved from 0.88260 to 0.88600, saving model to best.h5\n",
            "20/20 [==============================] - 8s 421ms/step - loss: 0.2004 - accuracy: 0.9251 - val_loss: 0.2923 - val_accuracy: 0.8860\n",
            "Epoch 14/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9368\n",
            "Epoch 00014: val_accuracy did not improve from 0.88600\n",
            "20/20 [==============================] - 8s 414ms/step - loss: 0.1778 - accuracy: 0.9368 - val_loss: 0.3303 - val_accuracy: 0.8718\n",
            "Epoch 15/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9355\n",
            "Epoch 00015: val_accuracy did not improve from 0.88600\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.1749 - accuracy: 0.9355 - val_loss: 0.2963 - val_accuracy: 0.8832\n",
            "Epoch 16/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9570\n",
            "Epoch 00016: val_accuracy did not improve from 0.88600\n",
            "20/20 [==============================] - 8s 416ms/step - loss: 0.1364 - accuracy: 0.9570 - val_loss: 0.3265 - val_accuracy: 0.8710\n",
            "Epoch 17/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9679\n",
            "Epoch 00017: val_accuracy did not improve from 0.88600\n",
            "20/20 [==============================] - 8s 418ms/step - loss: 0.1128 - accuracy: 0.9679 - val_loss: 0.3157 - val_accuracy: 0.8836\n",
            "Epoch 18/30\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9714\n",
            "Epoch 00018: val_accuracy did not improve from 0.88600\n",
            "20/20 [==============================] - 8s 416ms/step - loss: 0.0984 - accuracy: 0.9714 - val_loss: 0.3323 - val_accuracy: 0.8798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Model trainning complete\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 00018: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMDZPqIr7bZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "90c35070-b59a-4b0f-bfdb-61af5252d764"
      },
      "source": [
        "# validation & predict\n",
        "model.load_weights('best.h5')\n",
        "y_preds = model.predict(test_inputs).round().astype(int)\n",
        "logging.info(\"Prediction DONE\")\n",
        "\n",
        "\n",
        "sub = pd.read_csv('sample.csv')\n",
        "sub['sentiment'] = y_preds\n",
        "sub.to_csv('popcorn_submission.csv', index=False)!curl -X PUT --upload-file popcorn_submission.csv ali.140714.xyz:8000/\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Prediction DONE\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0  227k    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "\n",
            "## Received: \"popcorn_submission.csv\"\n",
            "\n",
            "100  227k    0    41  100  227k     25   142k  0:00:01  0:00:01 --:--:--  142k\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}