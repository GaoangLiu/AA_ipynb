{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis on Movie Reviews CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMy1h3sEtu37khgi/ENAJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Kzs5uF_Tgm",
        "colab_type": "text"
      },
      "source": [
        "Sentiment Analysis on Movie Reviews Bayes:\n",
        "[https://github.com/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_Naive_Bayes.ipynb](https://github.com/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_Naive_Bayes.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci7DHJ4j_g7Y",
        "colab_type": "text"
      },
      "source": [
        "## import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL5WEu3O_OfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b30d87f5-9d17-43b3-9a58-3c2a3459346c"
      },
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import timeit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import logging\n",
        "import time\n",
        "import smart_open\n",
        "import importlib\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "logging.basicConfig(format='[%(asctime)s %(levelname)8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
        "\n",
        "import keras\n",
        "from keras import layers, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow_hub as tfh\n",
        "\n",
        "!pip install arrow\n",
        "import arrow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n",
            "[05-18 02:46:21     INFO] 'pattern' package not found; tag filters are not available for English\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PclF2vR_vqB",
        "colab_type": "text"
      },
      "source": [
        "## Download files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfrkbIqG_zsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f3d09ff0-1ecf-4cf4-c397-234aa7092d00"
      },
      "source": [
        "! rm *.tsv *.zip *.csv\n",
        "! wget -O movie.zip ali.140714.xyz:8000/sentiment_analysis.zip \n",
        "! wget -O b7.py ali.140714.xyz:8000/boost117.py\n",
        "! unzip movie.zip \n",
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*.tsv': No such file or directory\n",
            "rm: cannot remove '*.zip': No such file or directory\n",
            "rm: cannot remove '*.csv': No such file or directory\n",
            "--2020-05-18 02:46:39--  http://ali.140714.xyz:8000/sentiment_analysis.zip\n",
            "Resolving ali.140714.xyz (ali.140714.xyz)... 47.240.16.188\n",
            "Connecting to ali.140714.xyz (ali.140714.xyz)|47.240.16.188|:8000... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1976135 (1.9M) [application/zip]\n",
            "Saving to: ‘movie.zip’\n",
            "\n",
            "movie.zip           100%[===================>]   1.88M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-18 02:46:39 (19.6 MB/s) - ‘movie.zip’ saved [1976135/1976135]\n",
            "\n",
            "--2020-05-18 02:46:42--  http://ali.140714.xyz:8000/boost117.py\n",
            "Resolving ali.140714.xyz (ali.140714.xyz)... 47.240.16.188\n",
            "Connecting to ali.140714.xyz (ali.140714.xyz)|47.240.16.188|:8000... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1723 (1.7K) [text/plain]\n",
            "Saving to: ‘b7.py’\n",
            "\n",
            "b7.py               100%[===================>]   1.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-18 02:46:42 (3.59 MB/s) - ‘b7.py’ saved [1723/1723]\n",
            "\n",
            "Archive:  movie.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: test.tsv                \n",
            "  inflating: train.tsv               \n",
            "b7.py  movie.zip  sample_data  sampleSubmission.csv  test.tsv  train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opg4CI3KBRqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.tsv', sep='\\t')\n",
        "train.Phrase.str.len().hist()\n",
        "train.Sentiment.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QcCvyjn_9-k",
        "colab_type": "text"
      },
      "source": [
        "## Tune models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfQdmKG5AFyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "7cd43cba-ca36-4cb0-e473-3dd6bb7f806f"
      },
      "source": [
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    self.train = None\n",
        "    self.test = None \n",
        "    self.model = None\n",
        "    \n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv('train.tsv', sep=\"\\t\")\n",
        "      self.train.rename({'Phrase': 'text', 'Sentiment': 'target'}, axis='columns', inplace=True)\n",
        "      self.test = pd.read_csv('test.tsv', sep=\"\\t\")\n",
        "      self.test.rename({'Phrase': 'text', 'Sentiment': 'target'}, axis='columns', inplace=True)\n",
        "      logging.info('TSV data loaded')\n",
        "  \n",
        "  def save_predictions(self, y_preds):\n",
        "      sub = pd.read_csv(f\"sampleSubmission.csv\")\n",
        "      sub['Sentiment'] = y_preds \n",
        "      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n",
        "      logging.info(f'Prediction exported to submission_{self.__class__.__name__}.csv')\n",
        "  \n",
        "\n",
        "class C_NN(Classifier):\n",
        "    def __init__(self, max_features=100000, embed_size=128, max_len=300):\n",
        "        self.max_features=max_features\n",
        "        self.embed_size=embed_size\n",
        "        self.max_len=max_len\n",
        "    \n",
        "    def tokenize_text(self, text_train, text_test):\n",
        "        '''@para: max_features, the most commenly used words in data set\n",
        "        @input are vector of text\n",
        "        '''\n",
        "        tokenizer = Tokenizer(num_words=self.max_features)\n",
        "        text = pd.concat([text_train, text_test])\n",
        "        tokenizer.fit_on_texts(text)\n",
        "\n",
        "        sequence_train = tokenizer.texts_to_sequences(text_train)\n",
        "        tokenized_train = pad_sequences(sequence_train, maxlen=self.max_len)\n",
        "        logging.info('Train text tokeninzed')\n",
        "\n",
        "        sequence_test = tokenizer.texts_to_sequences(text_test)\n",
        "        tokenized_test = pad_sequences(sequence_test, maxlen=self.max_len)\n",
        "        logging.info('Test text tokeninzed')\n",
        "        return tokenized_train, tokenized_test, tokenizer\n",
        "      \n",
        "    def build_model(self, embed_matrix=[]):\n",
        "        text_input = Input(shape=(self.max_len, ))\n",
        "        embed_text = layers.Embedding(self.max_features, self.embed_size)(text_input)\n",
        "        if len(embed_matrix) > 0:\n",
        "            embed_text = layers.Embedding(self.max_features, self.embed_size, \\\n",
        "                                          weights=[embed_matrix], trainable=False)(text_input)\n",
        "            \n",
        "        branch_a = layers.Bidirectional(layers.GRU(32, return_sequences=True))(embed_text)\n",
        "        branch_b = layers.GlobalMaxPool1D()(branch_a)\n",
        "\n",
        "        x = layers.Dense(64, activation='relu')(branch_b)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "        x = layers.Dense(32, activation='relu')(branch_b)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        branch_z = layers.Dense(5, activation='softmax')(x)\n",
        "        \n",
        "        model = Model(inputs=text_input, outputs=branch_z)\n",
        "        self.model = model\n",
        "\n",
        "        return model\n",
        "        \n",
        "    def embed_word_vector(self, word_index, model='glove-wiki-gigaword-100'):\n",
        "        glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "        zeros = [0] * self.embed_size\n",
        "        matrix = np.zeros((self.max_features, self.embed_size))\n",
        "          \n",
        "        for word, i in word_index.items(): \n",
        "            if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "            matrix[i] = glove[word]\n",
        "\n",
        "        logging.info('Matrix with embedded word vector created')\n",
        "        return matrix\n",
        "\n",
        "    def run(self, x_train, y_train):\n",
        "        checkpoint = ModelCheckpoint('weights_base_best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=3)\n",
        "\n",
        "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=2020)\n",
        "        BATCH_SIZE = max(16, 2 ** int(math.log(len(X_tra) / 100, 2)))\n",
        "        logging.info(f\"Batch size is set to {BATCH_SIZE}\")\n",
        "        history = self.model.fit(X_tra, y_tra, epochs=30, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), \\\n",
        "                              callbacks=[checkpoint, early], verbose=1)\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "c = C_NN(max_features=25000, embed_size=300, max_len=250)\n",
        "c.load_data()  \n",
        "labels = keras.utils.to_categorical(c.train.target, num_classes=5)      \n",
        "vector_train, vector_test, tokenizer = c.tokenize_text(c.train.text, c.test.text)\n",
        "\n",
        "embed = c.embed_word_vector(tokenizer.word_index, 'word2vec-google-news-300')\n",
        "c.build_model(embed_matrix=embed)\n",
        "c.run(vector_train, labels)\n",
        "# vector_train, labels\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-18 06:17:58     INFO] TSV data loaded\n",
            "[05-18 06:18:02     INFO] Train text tokeninzed\n",
            "[05-18 06:18:03     INFO] Test text tokeninzed\n",
            "[05-18 06:18:04     INFO] loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "[05-18 06:19:30     INFO] loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "[05-18 06:19:30     INFO] Matrix with embedded word vector created\n",
            "[05-18 06:19:30     INFO] Batch size is set to 1024\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 124848 samples, validate on 31212 samples\n",
            "Epoch 1/30\n",
            "124848/124848 [==============================] - 99s 796us/step - loss: 1.1058 - acc: 0.5704 - val_loss: 0.9309 - val_acc: 0.6129\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.61294, saving model to weights_base_best.hdf5\n",
            "Epoch 2/30\n",
            "124848/124848 [==============================] - 99s 794us/step - loss: 0.9307 - acc: 0.6134 - val_loss: 0.8865 - val_acc: 0.6283\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.61294 to 0.62828, saving model to weights_base_best.hdf5\n",
            "Epoch 3/30\n",
            "124848/124848 [==============================] - 98s 788us/step - loss: 0.8996 - acc: 0.6261 - val_loss: 0.8779 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.62828 to 0.63504, saving model to weights_base_best.hdf5\n",
            "Epoch 4/30\n",
            "118784/124848 [===========================>..] - ETA: 4s - loss: 0.8814 - acc: 0.6336"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klfy_GBpI8Xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5030cef8-0621-478d-edec-d93663b15423"
      },
      "source": [
        "# Make predictions\n",
        "model = load_model('weights_base_best.hdf5')\n",
        "y_preds = model.predict(vector_test)\n",
        "print(\"DONE\", arrow.now())\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE 2020-05-18T06:02:42.315509+00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU6imzL8N4DZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1620b784-09df-4a0e-94d6-84c03b445672"
      },
      "source": [
        "# Export submissions to csv file\n",
        "probs = np.argmax(y_preds, axis=1)\n",
        "sub = pd.read_csv('sampleSubmission.csv')\n",
        "sub['Sentiment'] = probs\n",
        "sub['Sentiment'].value_counts()\n",
        "\n",
        "export_file = 'submission_gru.csv'\n",
        "sub.to_csv(export_file, index=False)\n",
        "import b7 \n",
        "b7.Files().upload_vps(export_file)\n",
        "b7.Files().upload_vps('weights_base_best.hdf5')\n",
        "print(\"DONE \", arrow.now())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-18 06:02:44     INFO] submission_gru.csv was uploaded\n",
            "[05-18 06:02:45     INFO] weights_base_best.hdf5 was uploaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DONE  2020-05-18T06:02:45.957136+00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwOYjt8wud-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}