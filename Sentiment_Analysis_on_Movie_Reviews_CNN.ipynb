{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis on Movie Reviews CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ci7DHJ4j_g7Y"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOmUkvDLZEgsRo4gx4+5zKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Kzs5uF_Tgm",
        "colab_type": "text"
      },
      "source": [
        "Sentiment Analysis on Movie Reviews Bayes:\n",
        "[https://github.com/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_Naive_Bayes.ipynb](https://github.com/GaoangLiu/AA_ipynb/blob/master/Sentiment_Analysis_on_Movie_Reviews_Naive_Bayes.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci7DHJ4j_g7Y",
        "colab_type": "text"
      },
      "source": [
        "## import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL5WEu3O_OfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import timeit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import logging\n",
        "import time\n",
        "import smart_open\n",
        "import importlib\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "logging.basicConfig(format='[%(asctime)s %(levelname)8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
        "\n",
        "import keras\n",
        "from keras import layers, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow_hub as tfh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PclF2vR_vqB",
        "colab_type": "text"
      },
      "source": [
        "## Download files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfrkbIqG_zsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.tsv *.zip *.csv\n",
        "! wget -O movie.zip ali.140714.xyz:8000/sentiment_analysis.zip \n",
        "! wget -O b7.py ali.140714.xyz:8000/boost117.py\n",
        "! unzip movie.zip \n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opg4CI3KBRqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.tsv', sep='\\t')\n",
        "train.Phrase.str.len().hist()\n",
        "train.Sentiment.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QcCvyjn_9-k",
        "colab_type": "text"
      },
      "source": [
        "## Tune models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfQdmKG5AFyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32c38de7-6758-4c26-b6b8-927b7ee27cc0"
      },
      "source": [
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    self.train = None\n",
        "    self.test = None \n",
        "    self.model = None\n",
        "    \n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv('train.tsv', sep=\"\\t\")\n",
        "      self.train.rename({'Phrase': 'text', 'Sentiment': 'target'}, axis='columns', inplace=True)\n",
        "      self.test = pd.read_csv('test.tsv', sep=\"\\t\")\n",
        "      self.test.rename({'Phrase': 'text', 'Sentiment': 'target'}, axis='columns', inplace=True)\n",
        "      logging.info('TSV data loaded')\n",
        "  \n",
        "  def save_predictions(self, y_preds):\n",
        "      sub = pd.read_csv(f\"sampleSubmission.csv\")\n",
        "      sub['Sentiment'] = y_preds \n",
        "      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n",
        "      logging.info(f'Prediction exported to submission_{self.__class__.__name__}.csv')\n",
        "  \n",
        "\n",
        "class C_NN(Classifier):\n",
        "    def __init__(self, max_features=100000, embed_size=128, max_len=300):\n",
        "        self.max_features=max_features\n",
        "        self.embed_size=embed_size\n",
        "        self.max_len=max_len\n",
        "    \n",
        "    def tokenize_text(self, text_train, text_test):\n",
        "        '''@para: max_features, the most commenly used words in data set\n",
        "        @input are vector of text\n",
        "        '''\n",
        "        tokenizer = Tokenizer(num_words=self.max_features)\n",
        "        text = pd.concat([text_train, text_test])\n",
        "        tokenizer.fit_on_texts(text)\n",
        "\n",
        "        sequence_train = tokenizer.texts_to_sequences(text_train)\n",
        "        tokenized_train = pad_sequences(sequence_train, maxlen=self.max_len)\n",
        "        logging.info('Train text tokeninzed')\n",
        "\n",
        "        sequence_test = tokenizer.texts_to_sequences(text_test)\n",
        "        tokenized_test = pad_sequences(sequence_test, maxlen=self.max_len)\n",
        "        logging.info('Test text tokeninzed')\n",
        "        return tokenized_train, tokenized_test, tokenizer\n",
        "      \n",
        "    def build_model(self, embed_matrix=[]):\n",
        "        text_input = Input(shape=(self.max_len, ))\n",
        "        embed_text = layers.Embedding(self.max_features, self.embed_size)(text_input)\n",
        "        if len(embed_matrix) > 0:\n",
        "            embed_text = layers.Embedding(self.max_features, self.embed_size, \\\n",
        "                                          weights=[embed_matrix], trainable=False)(text_input)\n",
        "            \n",
        "        branch_a = layers.Bidirectional(layers.GRU(32, return_sequences=True))(embed_text)\n",
        "        branch_b = layers.GlobalMaxPool1D()(branch_a)\n",
        "\n",
        "        x = layers.Dense(64, activation='relu')(branch_b)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "        x = layers.Dense(32, activation='relu')(branch_b)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        branch_z = layers.Dense(5, activation='softmax')(x)\n",
        "        \n",
        "        model = Model(inputs=text_input, outputs=branch_z)\n",
        "        self.model = model\n",
        "\n",
        "        return model\n",
        "        \n",
        "    def embed_word_vector(self, word_index, model='glove-wiki-gigaword-100'):\n",
        "        glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "        zeros = [0] * self.embed_size\n",
        "        matrix = np.zeros((self.max_features, self.embed_size))\n",
        "          \n",
        "        for word, i in word_index.items(): \n",
        "            if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "            matrix[i] = glove[word]\n",
        "\n",
        "        logging.info('Matrix with embedded word vector created')\n",
        "        return matrix\n",
        "\n",
        "    def run(self, x_train, y_train):\n",
        "        checkpoint = ModelCheckpoint('weights_base_best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=3)\n",
        "\n",
        "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=2020)\n",
        "        BATCH_SIZE = max(16, 2 ** int(math.log(len(X_tra) / 100, 2)))\n",
        "        logging.info(f\"Batch size is set to {BATCH_SIZE}\")\n",
        "        history = self.model.fit(X_tra, y_tra, epochs=30, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), \\\n",
        "                              callbacks=[checkpoint, early], verbose=1)\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "c = C_NN(max_features=15000, embed_size=300, max_len=250)\n",
        "c.load_data()  \n",
        "labels = keras.utils.to_categorical(c.train.target, num_classes=5)      \n",
        "vector_train, vector_test, tokenizer = c.tokenize_text(c.train.text, c.test.text)\n",
        "\n",
        "embed = c.embed_word_vector(tokenizer.word_index, 'fasttext-wiki-news-subwords-300')\n",
        "c.build_model(embed_matrix=embed)\n",
        "c.run(vector_train, labels)\n",
        "# vector_train, labels\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-17 16:34:51     INFO] TSV data loaded\n",
            "[05-17 16:34:57     INFO] Train text tokeninzed\n",
            "[05-17 16:34:58     INFO] Test text tokeninzed\n",
            "[05-17 16:34:58     INFO] loading projection weights from /root/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "[05-17 16:40:09     INFO] loaded (999999, 300) matrix from /root/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\n",
            "[05-17 16:40:09     INFO] Matrix with embedded word vector created\n",
            "[05-17 16:40:09     INFO] Batch size is set to 1024\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 124848 samples, validate on 31212 samples\n",
            "Epoch 1/30\n",
            "124848/124848 [==============================] - 164s 1ms/step - loss: 1.2567 - acc: 0.5161 - val_loss: 1.0765 - val_acc: 0.5710\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.57100, saving model to weights_base_best.hdf5\n",
            "Epoch 2/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 1.0438 - acc: 0.5857 - val_loss: 0.9723 - val_acc: 0.6010\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.57100 to 0.60095, saving model to weights_base_best.hdf5\n",
            "Epoch 3/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.9796 - acc: 0.6018 - val_loss: 0.9303 - val_acc: 0.6150\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.60095 to 0.61502, saving model to weights_base_best.hdf5\n",
            "Epoch 4/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 0.9496 - acc: 0.6090 - val_loss: 0.9163 - val_acc: 0.6205\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.61502 to 0.62050, saving model to weights_base_best.hdf5\n",
            "Epoch 5/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.9342 - acc: 0.6150 - val_loss: 0.9031 - val_acc: 0.6268\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.62050 to 0.62681, saving model to weights_base_best.hdf5\n",
            "Epoch 6/30\n",
            "124848/124848 [==============================] - 158s 1ms/step - loss: 0.9209 - acc: 0.6214 - val_loss: 0.8993 - val_acc: 0.6291\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.62681 to 0.62905, saving model to weights_base_best.hdf5\n",
            "Epoch 7/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 0.9103 - acc: 0.6238 - val_loss: 0.8896 - val_acc: 0.6303\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.62905 to 0.63033, saving model to weights_base_best.hdf5\n",
            "Epoch 8/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.9031 - acc: 0.6260 - val_loss: 0.8865 - val_acc: 0.6308\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.63033 to 0.63078, saving model to weights_base_best.hdf5\n",
            "Epoch 9/30\n",
            "124848/124848 [==============================] - 155s 1ms/step - loss: 0.8970 - acc: 0.6289 - val_loss: 0.8839 - val_acc: 0.6346\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.63078 to 0.63460, saving model to weights_base_best.hdf5\n",
            "Epoch 10/30\n",
            "124848/124848 [==============================] - 158s 1ms/step - loss: 0.8894 - acc: 0.6323 - val_loss: 0.8754 - val_acc: 0.6368\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.63460 to 0.63684, saving model to weights_base_best.hdf5\n",
            "Epoch 11/30\n",
            "124848/124848 [==============================] - 157s 1ms/step - loss: 0.8841 - acc: 0.6339 - val_loss: 0.8721 - val_acc: 0.6360\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.63684\n",
            "Epoch 12/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 0.8773 - acc: 0.6373 - val_loss: 0.8677 - val_acc: 0.6387\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.63684 to 0.63870, saving model to weights_base_best.hdf5\n",
            "Epoch 13/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 0.8722 - acc: 0.6388 - val_loss: 0.8678 - val_acc: 0.6407\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.63870 to 0.64065, saving model to weights_base_best.hdf5\n",
            "Epoch 14/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.8684 - acc: 0.6396 - val_loss: 0.8633 - val_acc: 0.6432\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.64065 to 0.64315, saving model to weights_base_best.hdf5\n",
            "Epoch 15/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.8615 - acc: 0.6427 - val_loss: 0.8609 - val_acc: 0.6399\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.64315\n",
            "Epoch 16/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.8567 - acc: 0.6442 - val_loss: 0.8545 - val_acc: 0.6455\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.64315 to 0.64546, saving model to weights_base_best.hdf5\n",
            "Epoch 17/30\n",
            "124848/124848 [==============================] - 156s 1ms/step - loss: 0.8518 - acc: 0.6479 - val_loss: 0.8486 - val_acc: 0.6477\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.64546 to 0.64767, saving model to weights_base_best.hdf5\n",
            "Epoch 18/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 0.8476 - acc: 0.6480 - val_loss: 0.8502 - val_acc: 0.6447\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.64767\n",
            "Epoch 19/30\n",
            "124848/124848 [==============================] - 158s 1ms/step - loss: 0.8429 - acc: 0.6511 - val_loss: 0.8476 - val_acc: 0.6478\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.64767 to 0.64776, saving model to weights_base_best.hdf5\n",
            "Epoch 20/30\n",
            "124848/124848 [==============================] - 160s 1ms/step - loss: 0.8387 - acc: 0.6522 - val_loss: 0.8485 - val_acc: 0.6494\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.64776 to 0.64937, saving model to weights_base_best.hdf5\n",
            "Epoch 21/30\n",
            "124848/124848 [==============================] - 158s 1ms/step - loss: 0.8346 - acc: 0.6534 - val_loss: 0.8497 - val_acc: 0.6461\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.64937\n",
            "Epoch 22/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.8302 - acc: 0.6548 - val_loss: 0.8412 - val_acc: 0.6506\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.64937 to 0.65058, saving model to weights_base_best.hdf5\n",
            "Epoch 23/30\n",
            "124848/124848 [==============================] - 158s 1ms/step - loss: 0.8258 - acc: 0.6565 - val_loss: 0.8378 - val_acc: 0.6539\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.65058 to 0.65388, saving model to weights_base_best.hdf5\n",
            "Epoch 24/30\n",
            "124848/124848 [==============================] - 159s 1ms/step - loss: 0.8223 - acc: 0.6588 - val_loss: 0.8328 - val_acc: 0.6544\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.65388 to 0.65440, saving model to weights_base_best.hdf5\n",
            "Epoch 25/30\n",
            "124848/124848 [==============================] - 155s 1ms/step - loss: 0.8176 - acc: 0.6597 - val_loss: 0.8359 - val_acc: 0.6525\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.65440\n",
            "Epoch 26/30\n",
            "124848/124848 [==============================] - 156s 1ms/step - loss: 0.8130 - acc: 0.6615 - val_loss: 0.8437 - val_acc: 0.6515\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.65440\n",
            "Epoch 27/30\n",
            "124848/124848 [==============================] - 158s 1ms/step - loss: 0.8092 - acc: 0.6639 - val_loss: 0.8650 - val_acc: 0.6449\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.65440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6ccbbb5da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klfy_GBpI8Xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa93ba71-d445-43ad-9a72-ca2d1272e1b4"
      },
      "source": [
        "# Make predictions\n",
        "\n",
        "model = load_model('weights_base_best.hdf5')\n",
        "y_preds = model.predict(vector_test)\n",
        "print(\"DONE Good Morning\")\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE Good Morning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU6imzL8N4DZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "23e75eea-187c-4631-aea8-e8992f0f8995"
      },
      "source": [
        "# Export submissions to csv file\n",
        "probs = np.argmax(y_preds, axis=1)\n",
        "sub = pd.read_csv('sampleSubmission.csv')\n",
        "sub['Sentiment'] = probs\n",
        "sub['Sentiment'].value_counts()\n",
        "\n",
        "export_file = 'submission_gru.csv'\n",
        "sub.to_csv(export_file, index=False)\n",
        "import b7 \n",
        "b7.Files().upload_vps(export_file)\n",
        "b7.Files().upload_vps('weights_base_best.hdf5')\n",
        "print(\"DONE Good Morning\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-17 17:55:09     INFO] submission_gru.csv was uploaded\n",
            "[05-17 17:55:13     INFO] weights_base_best.hdf5 was uploaded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DONE Good Morning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwOYjt8wud-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}