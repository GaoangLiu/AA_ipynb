{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Toxic Comment Classification Challenge.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcc9OnXsYMRxOIF4cmBlX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/ipynb/blob/master/Toxic_Comment_Classification_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZZSHlxD37lH",
        "colab_type": "text"
      },
      "source": [
        "# What is this ?\n",
        "This is a kaggle contest [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
        "\n",
        "The contestants are required to build a multi-headed model that’s capable of **detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models**. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\n",
        "\n",
        "Refer blog \n",
        "[kernal](https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOgWCPr835V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data\n",
        "! rm *\n",
        "! wget -O data.zip bwg.140714.xyz:8000/toxic.zip \n",
        "! unzip data.zip \n",
        "! unzip train.csv.zip \n",
        "! unzip test.csv.zip \n",
        "! unzip test_labels.csv.zip \n",
        "! unzip sample_submission.csv.zip \n",
        "! ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQweAFgc4qvC",
        "colab_type": "text"
      },
      "source": [
        "Load necessary packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8SuenfI4s_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBY9NA_5Hxp",
        "colab_type": "text"
      },
      "source": [
        "Read and process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_mkjE7G5MjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = pd.read_csv('train.csv')\n",
        "# test  = pd.read_csv('test.csv')\n",
        "# sumb  = pd.read_csv('sample_submission.csv')\n",
        "# train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc2p6at-Z8-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "# train['good'] = 1-train[label_cols].max(axis=1)\n",
        "# train[train['comment_text'].str.len() < 15]\n",
        "\n",
        "# re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "# def tokenize(s): \n",
        "#   return re_tok.sub(r' \\1 ', s).split()\n",
        "\n",
        "# # for ct in train['comment_text']:\n",
        "# #   if len(ct) < 15:\n",
        "# #     print(ct, tokenize(ct))\n",
        "\n",
        "# # s = '“”¨«»®´·º½¾¿¡§£₤‘’'\n",
        "# # print(s, tokenize(s))\n",
        "# n = train.shape[0]\n",
        "# COMMENT = 'comment_text'\n",
        "# vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
        "#                min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
        "#                smooth_idf=1, sublinear_tf=1 )\n",
        "# trn_term_doc = vec.fit_transform(train[COMMENT])\n",
        "# test_term_doc = vec.transform(test[COMMENT])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-nK8r6gDMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "train = pd.read_csv('train.csv').fillna(' ')\n",
        "test = pd.read_csv('test.csv').fillna(' ')\n",
        "\n",
        "train_text = train['comment_text']\n",
        "test_text = test['comment_text']\n",
        "all_text = pd.concat([train_text, test_text])\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=1000)\n",
        "word_vectorizer.fit(all_text)\n",
        "train_word_features = word_vectorizer.transform(train_text)\n",
        "test_word_features = word_vectorizer.transform(test_text)\n",
        "\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char',\n",
        "    # stop_words='english',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=5000)\n",
        "char_vectorizer.fit(all_text)\n",
        "train_char_features = char_vectorizer.transform(train_text)\n",
        "test_char_features = char_vectorizer.transform(test_text)\n",
        "\n",
        "train_features = hstack([train_char_features, train_word_features])\n",
        "test_features = hstack([test_char_features, test_word_features])\n",
        "\n",
        "# train_features = train_word_features \n",
        "# test_features  = test_word_features \n",
        "\n",
        "scores = []\n",
        "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
        "\n",
        "    classifier.fit(train_features, train_target)\n",
        "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIsPpaf6viox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files \n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}