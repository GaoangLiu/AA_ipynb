{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_Insincere_Questions_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNJoJTzo+pPX9A65Un3W/QN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/ipynb/blob/master/Quora_Insincere_Questions_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3E1uAtsT17O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load packages \n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import timeit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import logging\n",
        "import time\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "logging.basicConfig(format='[%(asctime)s %(levelname)8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Get data\n",
        "! rm *.csv\n",
        "! wget -O quora.zip bwg.140714.xyz:8000/quora.zip \n",
        "! unzip quora.zip \n",
        "! ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bcd0v4GZiM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base class for classifier\n",
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    self.train = None\n",
        "    self.test = None \n",
        "    self.model = None\n",
        "\n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv(train_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      self.test = pd.read_csv(test_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      logging.info('CSV data loaded')\n",
        "  \n",
        "  def countvectorize(self):\n",
        "      tv = TfidfVectorizer(ngram_range=(1,3), token_pattern=r'\\w{1,}',\n",
        "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1, max_features=5000)\n",
        "      tv = CountVectorizer()\n",
        "      tv.fit(self.train.question_text)\n",
        "      self.vector_train = tv.transform(self.train.question_text)\n",
        "      self.vector_test  = tv.transform(self.test.question_text)\n",
        "      logging.info(\"Train & test text tokenized\")\n",
        "\n",
        "  def build_model(self):\n",
        "      pass\n",
        "\n",
        "  def run_model(self):\n",
        "      # Choose your own classifier: self.model and run it\n",
        "      logging.info(f\"{self.__class__.__name__} starts running.\")\n",
        "      labels = self.train.target\n",
        "      x_train, x_val, y_train, y_val = train_test_split(self.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "      self.model.fit(x_train, y_train)\n",
        "      y_preds = self.model.predict(x_val)\n",
        "\n",
        "      logging.info(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "      logging.info(f\"Confusion matrix: \") \n",
        "      print(confusion_matrix(y_val, y_preds))\n",
        "      print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))\n",
        "      # y_preds = self.model.predict(self.vector_test)\n",
        "      return y_preds\n",
        "\n",
        "  def save_predictions(self, y_preds):\n",
        "      sub = pd.read_csv(f\"sample_submission.csv\")\n",
        "      sub['prediction'] = y_preds \n",
        "      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n",
        "      logging.info('Prediction exported to submisison.csv')\n",
        "  \n",
        "  def pipeline(self):\n",
        "      s_time = time.clock()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "      self.build_model()\n",
        "      self.save_predictions(self.run_model())\n",
        "      logging.info(f\"Program running for {time.clock() - s_time} seconds\")\n",
        "\n",
        "class C_Bayes(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = MultinomialNB()\n",
        "      return self.model\n",
        "\n",
        "# Logistic Regression \n",
        "class C_LR(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = LogisticRegression(n_jobs=10, solver='lbfgs', C=0.1, verbose=1)\n",
        "      return self.model\n",
        "\n",
        "class C_SVM(Classifier):\n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv(train_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      self.train = self.train.sample(100000)\n",
        "      self.test = pd.read_csv(test_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      logging.info('CSV data loaded')\n",
        "\n",
        "  def build_model(self):\n",
        "      self.model = svm.SVC()\n",
        "      return self.model\n",
        "\n",
        "class C_Ensemble(Classifier):\n",
        "  def ensemble(self):\n",
        "      s_time = time.perf_counter()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "\n",
        "      nb = MultinomialNB()\n",
        "      lr = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n",
        "      svc = svm.SVC()\n",
        "\n",
        "      all_preds = [0] * self.test.shape[0]\n",
        "      for m in (nb, lr, svc):\n",
        "          self.model = m\n",
        "          if m == svc: \n",
        "              self.load_data()\n",
        "              self.train = self.train.sample(10000)\n",
        "              self.countvectorize()\n",
        "          all_preds += self.run_model()\n",
        "\n",
        "      all_preds = [1 if p > 0 else 0 for p in all_preds]\n",
        "      self.save_predictions(all_preds)\n",
        "      logging.info(f\"Program running for {time.perf_counter() - s_time} seconds\")\n",
        "\n",
        "\n",
        "class Helper():\n",
        "    def locate_threshold(self, model, x_val, y_val):\n",
        "        y_probs = model.predict_proba(x_val, batch_size=1024, verbose=1)\n",
        "        best_threshold = best_f1 = pre_f1 = 0\n",
        "        history = []\n",
        "\n",
        "        for i in np.arange(0.01, 1, 0.01):\n",
        "          if len(y_probs[0]) >= 2:\n",
        "              y2_preds = [1 if e[1] >= i else 0 for e in y_probs]\n",
        "          else:\n",
        "              y2_preds = (y_probs > i).astype(int)\n",
        "\n",
        "          cur_f1 = f1_score(y_val, y2_preds)\n",
        "          history.append((i, cur_f1))\n",
        "          symbol = '+' if cur_f1 >= pre_f1 else '-'\n",
        "          print(\"Threshold {:6.4f}, f1_score: {:<0.8f}  {} {:<0.6f} \".format(i, cur_f1, symbol, abs(cur_f1 - pre_f1)))\n",
        "          pre_f1 = cur_f1\n",
        "\n",
        "          if cur_f1 >= best_f1:\n",
        "              best_f1 = cur_f1\n",
        "              best_threshold = i\n",
        "\n",
        "        print(f\"Best f1 score {best_f1}, best threshold {best_threshold}\")\n",
        "        plt.xlabel('Threshold')\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.plot(*zip(*history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1mxAIjmVG_U",
        "colab_type": "text"
      },
      "source": [
        "## CNN\n",
        "We've tried linear models, the best result of private`f1_score` we got is 0.62166. Now we try Neural Network\n",
        "\n",
        "2020.05.11 Biderectional GRU achieved `private_score` 0.64255, `public_score` 0.63140\n",
        "\n",
        "Seems that, smaller `batch_size` produces better performance, both on `precision` and `f1_score` .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAe8YZhVVFKL",
        "colab_type": "code",
        "outputId": "73d5a983-5232-45b5-f4ac-ce4558d8fc80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class C_NN(Classifier):\n",
        "    def __init__(self, max_features=100000, embed_size=128, max_len=300):\n",
        "        self.max_features=max_features\n",
        "        self.embed_size=embed_size\n",
        "        self.max_len=max_len\n",
        "    \n",
        "    def tokenize_text(self, text_train, text_test):\n",
        "        '''@para: max_features, the most commenly used words in data set\n",
        "        @input are vector of text\n",
        "        '''\n",
        "        tokenizer = Tokenizer(num_words=self.max_features)\n",
        "        text = pd.concat([text_train, text_test])\n",
        "        tokenizer.fit_on_texts(text)\n",
        "\n",
        "        sequence_train = tokenizer.texts_to_sequences(text_train)\n",
        "        tokenized_train = pad_sequences(sequence_train, maxlen=self.max_len)\n",
        "        logging.info('Train text tokeninzed')\n",
        "\n",
        "        sequence_test = tokenizer.texts_to_sequences(text_test)\n",
        "        tokenized_test = pad_sequences(sequence_test, maxlen=self.max_len)\n",
        "        logging.info('Test text tokeninzed')\n",
        "        return tokenized_train, tokenized_test, tokenizer\n",
        "      \n",
        "    def build_model(self):\n",
        "        dropout = 0.2\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.max_features, self.embed_size, input_length=self.max_len))\n",
        "        model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model = model\n",
        "\n",
        "        return self.model\n",
        "    def embed_glove_vector(self, word_index, model='glove-wiki-gigaword-100'):\n",
        "        glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "        zeros = [0] * self.embed_size\n",
        "        matrix = np.zeros((self.max_features, self.embed_size))\n",
        "          \n",
        "        for word, i in word_index.items(): \n",
        "            if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "            matrix[i] = glove[word]\n",
        "\n",
        "        logging.info('Glove embedding vector created')\n",
        "        return matrix\n",
        "\n",
        "    def run(self, x_train, y_train):\n",
        "        checkpoint = ModelCheckpoint('weights_base_best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
        "\n",
        "        self.model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=2020)\n",
        "        BATCH_SIZE = max(16, 2 ** int(math.log(len(X_tra) / 100, 2)))\n",
        "        logging.info(f\"Batch size is set to {BATCH_SIZE}\")\n",
        "        history = self.model.fit(X_tra, y_tra, epochs=2, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), \\\n",
        "                              callbacks=[checkpoint, early], verbose=1)\n",
        "\n",
        "        y_pred = self.model.predict(X_val, batch_size=64, verbose=1)\n",
        "        y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "        print(classification_report(y_val, y_pred_bool))\n",
        "        return history\n",
        "\n",
        "c = C_NN(max_features=50000, embed_size=300, max_len=200)\n",
        "c.load_data()\n",
        "c.train = c.train[:300_000]\n",
        "# c.test = c.test.sample(1000)\n",
        "vector_train, vector_test, tokenizer = c.tokenize_text(c.train.question_text, c.test.question_text)\n",
        "model = c.build_model()\n",
        "\n",
        "# Embed pretrained word vector\n",
        "# embed_matrix = c.embed_glove_vector(tokenizer.word_index, model='word2vec-google-news-300')\n",
        "# model.layers[0].set_weights([embed_matrix])\n",
        "# model.layers[0].trainable = False"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-11 13:01:41     INFO] CSV data loaded\n",
            "[05-11 13:02:01     INFO] Train text tokeninzed\n",
            "[05-11 13:02:09     INFO] Test text tokeninzed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBqYC6fAgXEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmptrain = c.train\n",
        "tmptrain.question_text.str.len().hist()\n",
        "# ids = tmptrain[tmptrain.target == 0].sample(100).index\n",
        "# vector_train[ids]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR6_t3qnej2I",
        "colab_type": "code",
        "outputId": "435e0364-a197-4f78-caad-1dd38b44129a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "c.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# x_train, y_train = vector_train\n",
        "\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(vector_train, c.train.target, train_size=0.8, random_state=2020)\n",
        "history = c.model.fit(X_tra, y_tra, epochs=1, batch_size=1024, validation_data=(X_val, y_val))\n",
        "\n",
        "# y_pred = c.model.predict(X_val, batch_size=1024, verbose=1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 240000 samples, validate on 60000 samples\n",
            "Epoch 1/1\n",
            "240000/240000 [==============================] - 772s 3ms/step - loss: 0.1334 - acc: 0.9473 - val_loss: 0.1226 - val_acc: 0.9518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg1HsYkOsWMv",
        "colab_type": "code",
        "outputId": "a2df6c6d-b787-45ce-85ac-9ac0e54975b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Find maximum threshold \n",
        "Helper().locate_threshold(c.model, X_val, y_val)\n",
        "# 300_000 train size, batch_size = 1024, f1_score = 0.6118\n",
        "# 300_000 train size, batch_size = 1024, double GRU, f1_score = 0.6113\n",
        "# 300_000 train size, batch_size = 1024, with glove-wiki-gigaword-100 embeding, f1_score = 0.6030\n",
        "# 300_000 train size, batch_size = 1024, with google-news 300 embeding, f1_score = 0.6078\n",
        "# full train, bsize 1024, embed_size=300, maxlen=200, google news 300, f1_score = 0.6404\n",
        "# full train, bsize 1024, embed_size=300, maxlen=200, no embed, f1_score = 0.6390\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 51s 842us/step\n",
            "Threshold 0.0100, f1_score: 0.31470194  + 0.314702 \n",
            "Threshold 0.0200, f1_score: 0.38574309  + 0.071041 \n",
            "Threshold 0.0300, f1_score: 0.42870422  + 0.042961 \n",
            "Threshold 0.0400, f1_score: 0.45812841  + 0.029424 \n",
            "Threshold 0.0500, f1_score: 0.48137248  + 0.023244 \n",
            "Threshold 0.0600, f1_score: 0.49744053  + 0.016068 \n",
            "Threshold 0.0700, f1_score: 0.51243040  + 0.014990 \n",
            "Threshold 0.0800, f1_score: 0.52583685  + 0.013406 \n",
            "Threshold 0.0900, f1_score: 0.53641774  + 0.010581 \n",
            "Threshold 0.1000, f1_score: 0.54499957  + 0.008582 \n",
            "Threshold 0.1100, f1_score: 0.55287570  + 0.007876 \n",
            "Threshold 0.1200, f1_score: 0.55937726  + 0.006502 \n",
            "Threshold 0.1300, f1_score: 0.56643874  + 0.007061 \n",
            "Threshold 0.1400, f1_score: 0.57196755  + 0.005529 \n",
            "Threshold 0.1500, f1_score: 0.57656791  + 0.004600 \n",
            "Threshold 0.1600, f1_score: 0.57980647  + 0.003239 \n",
            "Threshold 0.1700, f1_score: 0.58388060  + 0.004074 \n",
            "Threshold 0.1800, f1_score: 0.58752149  + 0.003641 \n",
            "Threshold 0.1900, f1_score: 0.59170867  + 0.004187 \n",
            "Threshold 0.2000, f1_score: 0.59508111  + 0.003372 \n",
            "Threshold 0.2100, f1_score: 0.59732541  + 0.002244 \n",
            "Threshold 0.2200, f1_score: 0.60023750  + 0.002912 \n",
            "Threshold 0.2300, f1_score: 0.60280272  + 0.002565 \n",
            "Threshold 0.2400, f1_score: 0.60459438  + 0.001792 \n",
            "Threshold 0.2500, f1_score: 0.60651685  + 0.001922 \n",
            "Threshold 0.2600, f1_score: 0.60913014  + 0.002613 \n",
            "Threshold 0.2700, f1_score: 0.60902515  - 0.000105 \n",
            "Threshold 0.2800, f1_score: 0.61022067  + 0.001196 \n",
            "Threshold 0.2900, f1_score: 0.61062051  + 0.000400 \n",
            "Threshold 0.3000, f1_score: 0.61133603  + 0.000716 \n",
            "Threshold 0.3100, f1_score: 0.61000240  - 0.001334 \n",
            "Threshold 0.3200, f1_score: 0.61099648  + 0.000994 \n",
            "Threshold 0.3300, f1_score: 0.61014457  - 0.000852 \n",
            "Threshold 0.3400, f1_score: 0.60886797  - 0.001277 \n",
            "Threshold 0.3500, f1_score: 0.60773757  - 0.001130 \n",
            "Threshold 0.3600, f1_score: 0.60802625  + 0.000289 \n",
            "Threshold 0.3700, f1_score: 0.60725652  - 0.000770 \n",
            "Threshold 0.3800, f1_score: 0.60760144  + 0.000345 \n",
            "Threshold 0.3900, f1_score: 0.60544041  - 0.002161 \n",
            "Threshold 0.4000, f1_score: 0.60583083  + 0.000390 \n",
            "Threshold 0.4100, f1_score: 0.60253031  - 0.003301 \n",
            "Threshold 0.4200, f1_score: 0.60207612  - 0.000454 \n",
            "Threshold 0.4300, f1_score: 0.60032189  - 0.001754 \n",
            "Threshold 0.4400, f1_score: 0.59891746  - 0.001404 \n",
            "Threshold 0.4500, f1_score: 0.59754098  - 0.001376 \n",
            "Threshold 0.4600, f1_score: 0.59342178  - 0.004119 \n",
            "Threshold 0.4700, f1_score: 0.59259259  - 0.000829 \n",
            "Threshold 0.4800, f1_score: 0.58907965  - 0.003513 \n",
            "Threshold 0.4900, f1_score: 0.58664388  - 0.002436 \n",
            "Threshold 0.5000, f1_score: 0.58320127  - 0.003443 \n",
            "Threshold 0.5100, f1_score: 0.57932446  - 0.003877 \n",
            "Threshold 0.5200, f1_score: 0.57557365  - 0.003751 \n",
            "Threshold 0.5300, f1_score: 0.56966596  - 0.005908 \n",
            "Threshold 0.5400, f1_score: 0.56313054  - 0.006535 \n",
            "Threshold 0.5500, f1_score: 0.55368814  - 0.009442 \n",
            "Threshold 0.5600, f1_score: 0.54397706  - 0.009711 \n",
            "Threshold 0.5700, f1_score: 0.53320281  - 0.010774 \n",
            "Threshold 0.5800, f1_score: 0.51467875  - 0.018524 \n",
            "Threshold 0.5900, f1_score: 0.49931034  - 0.015368 \n",
            "Threshold 0.6000, f1_score: 0.47310296  - 0.026207 \n",
            "Threshold 0.6100, f1_score: 0.45262965  - 0.020473 \n",
            "Threshold 0.6200, f1_score: 0.43142427  - 0.021205 \n",
            "Threshold 0.6300, f1_score: 0.40233010  - 0.029094 \n",
            "Threshold 0.6400, f1_score: 0.37584695  - 0.026483 \n",
            "Threshold 0.6500, f1_score: 0.34963325  - 0.026214 \n",
            "Threshold 0.6600, f1_score: 0.32659020  - 0.023043 \n",
            "Threshold 0.6700, f1_score: 0.30464423  - 0.021946 \n",
            "Threshold 0.6800, f1_score: 0.27613240  - 0.028512 \n",
            "Threshold 0.6900, f1_score: 0.25720621  - 0.018926 \n",
            "Threshold 0.7000, f1_score: 0.23767725  - 0.019529 \n",
            "Threshold 0.7100, f1_score: 0.22060164  - 0.017076 \n",
            "Threshold 0.7200, f1_score: 0.20004631  - 0.020555 \n",
            "Threshold 0.7300, f1_score: 0.17959567  - 0.020451 \n",
            "Threshold 0.7400, f1_score: 0.16198190  - 0.017614 \n",
            "Threshold 0.7500, f1_score: 0.14468290  - 0.017299 \n",
            "Threshold 0.7600, f1_score: 0.13180934  - 0.012874 \n",
            "Threshold 0.7700, f1_score: 0.12058824  - 0.011221 \n",
            "Threshold 0.7800, f1_score: 0.10255140  - 0.018037 \n",
            "Threshold 0.7900, f1_score: 0.08650000  - 0.016051 \n",
            "Threshold 0.8000, f1_score: 0.07788945  - 0.008611 \n",
            "Threshold 0.8100, f1_score: 0.06967937  - 0.008210 \n",
            "Threshold 0.8200, f1_score: 0.06240487  - 0.007275 \n",
            "Threshold 0.8300, f1_score: 0.05504587  - 0.007359 \n",
            "Threshold 0.8400, f1_score: 0.04562933  - 0.009417 \n",
            "Threshold 0.8500, f1_score: 0.03759011  - 0.008039 \n",
            "Threshold 0.8600, f1_score: 0.03254132  - 0.005049 \n",
            "Threshold 0.8700, f1_score: 0.02847528  - 0.004066 \n",
            "Threshold 0.8800, f1_score: 0.02338877  - 0.005087 \n",
            "Threshold 0.8900, f1_score: 0.01824818  - 0.005141 \n",
            "Threshold 0.9000, f1_score: 0.01618376  - 0.002064 \n",
            "Threshold 0.9100, f1_score: 0.01255887  - 0.003625 \n",
            "Threshold 0.9200, f1_score: 0.01151832  - 0.001041 \n",
            "Threshold 0.9300, f1_score: 0.00943644  - 0.002082 \n",
            "Threshold 0.9400, f1_score: 0.00525486  - 0.004182 \n",
            "Threshold 0.9500, f1_score: 0.00315623  - 0.002099 \n",
            "Threshold 0.9600, f1_score: 0.00210526  - 0.001051 \n",
            "Threshold 0.9700, f1_score: 0.00105319  - 0.001052 \n",
            "Threshold 0.9800, f1_score: 0.00000000  - 0.001053 \n",
            "Threshold 0.9900, f1_score: 0.00000000  + 0.000000 \n",
            "Best f1 score 0.6113360323886639, best threshold 0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAp-h_Ep8E6",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression \n",
        "Besides NB, we also tried LR to maximum `f1_score`\n",
        "\n",
        "2020.05.09: \n",
        "* Set `threshold = 0.21`, we got public score 0.60627 + private score = 0.62161. Bot are the maximum score we've got by now.\n",
        "<img class=\"center\" src=\"https://i.loli.net/2020/05/09/eE4KqloW52FDSdc.png\" alt=\"LR max f1_score\" width=850>\n",
        "* Set `threshold=0.25`, `public_score=0.60619` decreased, but `private_score=0.62166` increased. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4iCMLKZr68V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = C_LR()\n",
        "c.load_data()\n",
        "# b.train = b.train.sample(100000)\n",
        "c.countvectorize()\n",
        "labels = c.train.target\n",
        "x_train, x_val, y_train, y_val = train_test_split(c.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "\n",
        "model = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n",
        "model.fit(x_train, y_train)\n",
        "y_preds = model.predict(x_val)\n",
        "\n",
        "print(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "print(f\"Confusion matrix: \") \n",
        "print(confusion_matrix(y_val, y_preds))\n",
        "print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBA9t30Hs4jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the best threshold to maximum f1_score\n",
        "Helper().locate_threshold(model, x_val, y_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b421RTCRhymn",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "Since in this contest submissions are evaluated on **F1 Score** between the predicted and the observed targets. Our ultimate goal is to maximum the `f1_socre`. \n",
        "\n",
        "2020.05.09: \n",
        "* Naive Bayes achieved the maximum score `f1_score = 0.56456695` when the `threshold` is set to `0.726`. This result in a public score = 0.56452 + private score = 0.56706. The public socre is the best we've got with NB, but the private score is only second to 0.56889, one when we set the `threshold` to 0.6 .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wchbncDhALOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = C_Bayes()\n",
        "b.load_data()\n",
        "b.countvectorize()\n",
        "b.build_model()\n",
        "labels = b.train.target\n",
        "x_train, x_val, y_train, y_val = train_test_split(b.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "b.model.fit(x_train, y_train)\n",
        "y_preds = b.model.predict(x_val)\n",
        "\n",
        "logging.info(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "logging.info(f\"Confusion matrix: \") \n",
        "print(confusion_matrix(y_val, y_preds))\n",
        "print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_qcaj8UAsC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the best threshold to maximum f1_score\n",
        "Helper().locate_threshold(b.model, x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}