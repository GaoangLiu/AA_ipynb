{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_Insincere_Questions_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMlKB60mfqvZqQzVo8Dcebz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/ipynb/blob/master/Quora_Insincere_Questions_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3E1uAtsT17O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load packages \n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import timeit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import logging\n",
        "import time\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "logging.basicConfig(format='[%(asctime)s %(levelname)8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_AjiQTkGDGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data\n",
        "! rm *.csv\n",
        "! wget -O quora.zip bwg.140714.xyz:8000/quora.zip \n",
        "! unzip quora.zip \n",
        "! ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bcd0v4GZiM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base class for classifier\n",
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    self.train = None\n",
        "    self.test = None \n",
        "    self.model = None\n",
        "\n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv(train_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      self.test = pd.read_csv(test_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      logging.info('CSV data loaded')\n",
        "  \n",
        "  def countvectorize(self):\n",
        "      tv = TfidfVectorizer(ngram_range=(1,3), token_pattern=r'\\w{1,}',\n",
        "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1, max_features=5000)\n",
        "      tv = CountVectorizer()\n",
        "      tv.fit(self.train.question_text)\n",
        "      self.vector_train = tv.transform(self.train.question_text)\n",
        "      self.vector_test  = tv.transform(self.test.question_text)\n",
        "      logging.info(\"Train & test text tokenized\")\n",
        "\n",
        "  def build_model(self):\n",
        "      pass\n",
        "\n",
        "  def run_model(self):\n",
        "      # Choose your own classifier: self.model and run it\n",
        "      logging.info(f\"{self.__class__.__name__} starts running.\")\n",
        "      labels = self.train.target\n",
        "      x_train, x_val, y_train, y_val = train_test_split(self.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "      self.model.fit(x_train, y_train)\n",
        "      y_preds = self.model.predict(x_val)\n",
        "\n",
        "      logging.info(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "      logging.info(f\"Confusion matrix: \") \n",
        "      print(confusion_matrix(y_val, y_preds))\n",
        "      print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))\n",
        "      # y_preds = self.model.predict(self.vector_test)\n",
        "      return y_preds\n",
        "\n",
        "  def save_predictions(self, y_preds):\n",
        "      sub = pd.read_csv(f\"sample_submission.csv\")\n",
        "      sub['prediction'] = y_preds \n",
        "      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n",
        "      logging.info('Prediction exported to submisison.csv')\n",
        "  \n",
        "  def pipeline(self):\n",
        "      s_time = time.clock()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "      self.build_model()\n",
        "      self.save_predictions(self.run_model())\n",
        "      logging.info(f\"Program running for {time.clock() - s_time} seconds\")\n",
        "\n",
        "class C_Bayes(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = MultinomialNB()\n",
        "      return self.model\n",
        "\n",
        "# Logistic Regression \n",
        "class C_LR(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = LogisticRegression(n_jobs=10, solver='lbfgs', C=0.1, verbose=1)\n",
        "      return self.model\n",
        "\n",
        "class C_SVM(Classifier):\n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv(train_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      self.train = self.train.sample(100000)\n",
        "      self.test = pd.read_csv(test_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      logging.info('CSV data loaded')\n",
        "\n",
        "  def build_model(self):\n",
        "      self.model = svm.SVC()\n",
        "      return self.model\n",
        "\n",
        "class C_Ensemble(Classifier):\n",
        "  def ensemble(self):\n",
        "      s_time = time.perf_counter()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "\n",
        "      nb = MultinomialNB()\n",
        "      lr = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n",
        "      svc = svm.SVC()\n",
        "\n",
        "      all_preds = [0] * self.test.shape[0]\n",
        "      for m in (nb, lr, svc):\n",
        "          self.model = m\n",
        "          if m == svc: \n",
        "              self.load_data()\n",
        "              self.train = self.train.sample(10000)\n",
        "              self.countvectorize()\n",
        "          all_preds += self.run_model()\n",
        "\n",
        "      all_preds = [1 if p > 0 else 0 for p in all_preds]\n",
        "      self.save_predictions(all_preds)\n",
        "      logging.info(f\"Program running for {time.perf_counter() - s_time} seconds\")\n",
        "\n",
        "\n",
        "class Helper():\n",
        "    def locate_threshold(self, model, x_val, y_val):\n",
        "        y_probs = model.predict(x_val, batch_size=1024, verbose=1)\n",
        "        best_threshold = best_f1 = pre_f1 = 0\n",
        "        history = []\n",
        "\n",
        "        for i in np.arange(0.01, 1, 0.01):\n",
        "          if len(y_probs[0]) >= 2:\n",
        "              y2_preds = [1 if e[1] >= i else 0 for e in y_probs]\n",
        "          else:\n",
        "              y2_preds = (y_probs > i).astype(int)\n",
        "\n",
        "          cur_f1 = f1_score(y_val, y2_preds)\n",
        "          history.append((i, cur_f1))\n",
        "          symbol = '+' if cur_f1 >= pre_f1 else '-'\n",
        "          print(\"Threshold {:6.4f}, f1_score: {:<0.8f}  {} {:<0.6f} \".format(i, cur_f1, symbol, abs(cur_f1 - pre_f1)))\n",
        "          pre_f1 = cur_f1\n",
        "\n",
        "          if cur_f1 >= best_f1:\n",
        "              best_f1 = cur_f1\n",
        "              best_threshold = i\n",
        "\n",
        "        print(f\"Best f1 score {best_f1}, best threshold {best_threshold}\")\n",
        "        plt.xlabel('Threshold')\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.plot(*zip(*history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1mxAIjmVG_U",
        "colab_type": "text"
      },
      "source": [
        "## CNN\n",
        "We've tried linear models, the best result of private`f1_score` we got is 0.62166. Now we try Neural Network\n",
        "\n",
        "2020.05.11 \n",
        "\n",
        "* Biderectional GRU achieved `private_score` 0.64255, `public_score` 0.63140\n",
        "\n",
        "Seems that, smaller `batch_size` produces better performance, both on `precision` and `f1_score` .\n",
        "\n",
        "2020.05.12 Keras API GRU. \n",
        "\n",
        "* YES !!!! Finally reach `private_score=0.65443` and `public_score=0.63905` with Bidrectional GRU `max_features=50000, embed_size=300, max_len=100`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAe8YZhVVFKL",
        "colab_type": "code",
        "outputId": "e523c3d2-a608-42e1-ef32-960bc580e420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "class C_NN(Classifier):\n",
        "    def __init__(self, max_features=100000, embed_size=128, max_len=300):\n",
        "        self.max_features=max_features\n",
        "        self.embed_size=embed_size\n",
        "        self.max_len=max_len\n",
        "    \n",
        "    def tokenize_text(self, text_train, text_test):\n",
        "        '''@para: max_features, the most commenly used words in data set\n",
        "        @input are vector of text\n",
        "        '''\n",
        "        tokenizer = Tokenizer(num_words=self.max_features)\n",
        "        text = pd.concat([text_train, text_test])\n",
        "        tokenizer.fit_on_texts(text)\n",
        "\n",
        "        sequence_train = tokenizer.texts_to_sequences(text_train)\n",
        "        tokenized_train = pad_sequences(sequence_train, maxlen=self.max_len)\n",
        "        logging.info('Train text tokeninzed')\n",
        "\n",
        "        sequence_test = tokenizer.texts_to_sequences(text_test)\n",
        "        tokenized_test = pad_sequences(sequence_test, maxlen=self.max_len)\n",
        "        logging.info('Test text tokeninzed')\n",
        "        return tokenized_train, tokenized_test, tokenizer\n",
        "      \n",
        "    def build_model(self):\n",
        "        dropout = 0.2\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.max_features, self.embed_size, input_length=self.max_len))\n",
        "        model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model = model\n",
        "\n",
        "        return self.model\n",
        "        \n",
        "    def embed_word_vector(self, word_index, model='glove-wiki-gigaword-100'):\n",
        "        glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "        zeros = [0] * self.embed_size\n",
        "        matrix = np.zeros((self.max_features, self.embed_size))\n",
        "          \n",
        "        for word, i in word_index.items(): \n",
        "            if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "            matrix[i] = glove[word]\n",
        "\n",
        "        logging.info('Matrix with embedded word vector created')\n",
        "        return matrix\n",
        "\n",
        "    def run(self, x_train, y_train):\n",
        "        checkpoint = ModelCheckpoint('weights_base_best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
        "\n",
        "        self.model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=2020)\n",
        "        BATCH_SIZE = max(16, 2 ** int(math.log(len(X_tra) / 100, 2)))\n",
        "        logging.info(f\"Batch size is set to {BATCH_SIZE}\")\n",
        "        history = self.model.fit(X_tra, y_tra, epochs=2, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), \\\n",
        "                              callbacks=[checkpoint, early], verbose=1)\n",
        "\n",
        "        y_pred = self.model.predict(X_val, batch_size=64, verbose=1)\n",
        "        y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "        print(classification_report(y_val, y_pred_bool))\n",
        "        return history\n",
        "\n",
        "class C_GRU():\n",
        "    def __init__(self):\n",
        "        self.embed_size=300\n",
        "        self.max_features=50000\n",
        "        self.max_len=200\n",
        "    \n",
        "    def build_model(self, embed_matrix=[]):\n",
        "        text_input = Input(shape=(self.max_len, ))\n",
        "        embed_text = layers.Embedding(self.max_features, self.embed_size)(text_input)\n",
        "        if len(embed_matrix) > 0:\n",
        "            embed_text = layers.Embedding(self.max_features, self.embed_size, \\\n",
        "                                          weights=[embed_matrix], trainable=False)(text_input)\n",
        "            \n",
        "\n",
        "        branch_a = layers.Bidirectional(layers.GRU(64, return_sequences=True))(embed_text)\n",
        "        branch_b = layers.GlobalMaxPool1D()(branch_a)\n",
        "        branch_c = layers.Dense(64, activation='relu')(branch_b)\n",
        "        branch_d = layers.Dropout(0.1)(branch_c)\n",
        "        branch_z = layers.Dense(1, activation='sigmoid')(branch_d)\n",
        "        \n",
        "        model = Model(inputs=text_input, outputs=branch_z)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        \n",
        "        return model\n",
        "\n",
        "c = C_NN(max_features=50000, embed_size=300, max_len=200)\n",
        "c.load_data()\n",
        "# # c.train = c.train[:300_000]\n",
        "vector_train, vector_test, tokenizer = c.tokenize_text(c.train.question_text, c.test.question_text)\n",
        "matrix = c.embed_word_vector(tokenizer.word_index, model='word2vec-google-news-300')\n",
        "model = C_GRU().build_model(matrix)\n",
        "\n",
        "# model.get_weights()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-13 02:20:39     INFO] CSV data loaded\n",
            "[05-13 02:21:34     INFO] Train text tokeninzed\n",
            "[05-13 02:21:42     INFO] Test text tokeninzed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 98.9% 1644.0/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[05-13 02:24:45     INFO] word2vec-google-news-300 downloaded\n",
            "[05-13 02:24:45     INFO] loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "[05-13 02:26:21     INFO] loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "[05-13 02:26:21     INFO] Matrix with embedded word vector created\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR6_t3qnej2I",
        "colab_type": "code",
        "outputId": "469acaa4-6d89-4f2c-9c84-804d97151592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(vector_train, c.train.target, train_size=0.8, random_state=2020)\n",
        "history = model.fit(X_tra, y_tra, epochs=1, batch_size=256, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1044897 samples, validate on 261225 samples\n",
            "Epoch 1/1\n",
            " 806144/1044897 [======================>.......] - ETA: 10:26 - loss: 0.1222 - acc: 0.9532"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg1HsYkOsWMv",
        "colab_type": "code",
        "outputId": "7a8cb9f4-9649-4bab-fe1e-b16a95c6d9f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Find maximum threshold \n",
        "Helper().locate_threshold(model, X_val, y_val)\n",
        "# 300_000 train size, batch_size = 1024, f1_score = 0.6118\n",
        "# 300_000 train size, batch_size = 1024, double GRU, f1_score = 0.6113\n",
        "# 300_000 train size, batch_size = 1024, with glove-wiki-gigaword-100 embeding, f1_score = 0.6030\n",
        "# 300_000 train size, batch_size = 1024, with google-news 300 embeding, f1_score = 0.6078\n",
        "# full train, bsize 1024, embed_size=300, maxlen=200, google news 300, f1_score = 0.6404\n",
        "# full train, bsize 1024, embed_size=300, maxlen=200, no embed, f1_score = 0.6390\n",
        "\n",
        "# Keras API full train, Bidirectional GRU 64, bsize 256, embed_size 300, maxlen 200, no embed, f1_score 0.6485\n",
        "# Keras API full train, Bidirectional GRU 64, bsize 256, embed_size 300, maxlen 200, fasttext 300, f1_score 0.6417\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "261225/261225 [==============================] - 18s 67us/step\n",
            "Threshold 0.0100, f1_score: 0.29652138  + 0.296521 \n",
            "Threshold 0.0200, f1_score: 0.37186838  + 0.075347 \n",
            "Threshold 0.0300, f1_score: 0.41896770  + 0.047099 \n",
            "Threshold 0.0400, f1_score: 0.45350158  + 0.034534 \n",
            "Threshold 0.0500, f1_score: 0.47995640  + 0.026455 \n",
            "Threshold 0.0600, f1_score: 0.50089821  + 0.020942 \n",
            "Threshold 0.0700, f1_score: 0.51884685  + 0.017949 \n",
            "Threshold 0.0800, f1_score: 0.53318596  + 0.014339 \n",
            "Threshold 0.0900, f1_score: 0.54562763  + 0.012442 \n",
            "Threshold 0.1000, f1_score: 0.55586291  + 0.010235 \n",
            "Threshold 0.1100, f1_score: 0.56515084  + 0.009288 \n",
            "Threshold 0.1200, f1_score: 0.57346901  + 0.008318 \n",
            "Threshold 0.1300, f1_score: 0.58053748  + 0.007068 \n",
            "Threshold 0.1400, f1_score: 0.58760282  + 0.007065 \n",
            "Threshold 0.1500, f1_score: 0.59322258  + 0.005620 \n",
            "Threshold 0.1600, f1_score: 0.59913397  + 0.005911 \n",
            "Threshold 0.1700, f1_score: 0.60497067  + 0.005837 \n",
            "Threshold 0.1800, f1_score: 0.60913800  + 0.004167 \n",
            "Threshold 0.1900, f1_score: 0.61397588  + 0.004838 \n",
            "Threshold 0.2000, f1_score: 0.61796218  + 0.003986 \n",
            "Threshold 0.2100, f1_score: 0.62152888  + 0.003567 \n",
            "Threshold 0.2200, f1_score: 0.62537981  + 0.003851 \n",
            "Threshold 0.2300, f1_score: 0.62762575  + 0.002246 \n",
            "Threshold 0.2400, f1_score: 0.63088579  + 0.003260 \n",
            "Threshold 0.2500, f1_score: 0.63256462  + 0.001679 \n",
            "Threshold 0.2600, f1_score: 0.63459677  + 0.002032 \n",
            "Threshold 0.2700, f1_score: 0.63613947  + 0.001543 \n",
            "Threshold 0.2800, f1_score: 0.63798898  + 0.001850 \n",
            "Threshold 0.2900, f1_score: 0.63874431  + 0.000755 \n",
            "Threshold 0.3000, f1_score: 0.63943289  + 0.000689 \n",
            "Threshold 0.3100, f1_score: 0.63981003  + 0.000377 \n",
            "Threshold 0.3200, f1_score: 0.64028045  + 0.000470 \n",
            "Threshold 0.3300, f1_score: 0.64173848  + 0.001458 \n",
            "Threshold 0.3400, f1_score: 0.64119881  - 0.000540 \n",
            "Threshold 0.3500, f1_score: 0.64063829  - 0.000561 \n",
            "Threshold 0.3600, f1_score: 0.63984083  - 0.000797 \n",
            "Threshold 0.3700, f1_score: 0.63958659  - 0.000254 \n",
            "Threshold 0.3800, f1_score: 0.63900970  - 0.000577 \n",
            "Threshold 0.3900, f1_score: 0.63765706  - 0.001353 \n",
            "Threshold 0.4000, f1_score: 0.63481675  - 0.002840 \n",
            "Threshold 0.4100, f1_score: 0.63273163  - 0.002085 \n",
            "Threshold 0.4200, f1_score: 0.63052235  - 0.002209 \n",
            "Threshold 0.4300, f1_score: 0.62710931  - 0.003413 \n",
            "Threshold 0.4400, f1_score: 0.62278830  - 0.004321 \n",
            "Threshold 0.4500, f1_score: 0.61946726  - 0.003321 \n",
            "Threshold 0.4600, f1_score: 0.61446797  - 0.004999 \n",
            "Threshold 0.4700, f1_score: 0.61006890  - 0.004399 \n",
            "Threshold 0.4800, f1_score: 0.60520585  - 0.004863 \n",
            "Threshold 0.4900, f1_score: 0.59910194  - 0.006104 \n",
            "Threshold 0.5000, f1_score: 0.59284522  - 0.006257 \n",
            "Threshold 0.5100, f1_score: 0.58721139  - 0.005634 \n",
            "Threshold 0.5200, f1_score: 0.57911819  - 0.008093 \n",
            "Threshold 0.5300, f1_score: 0.57234655  - 0.006772 \n",
            "Threshold 0.5400, f1_score: 0.56336656  - 0.008980 \n",
            "Threshold 0.5500, f1_score: 0.55508684  - 0.008280 \n",
            "Threshold 0.5600, f1_score: 0.54607192  - 0.009015 \n",
            "Threshold 0.5700, f1_score: 0.53793157  - 0.008140 \n",
            "Threshold 0.5800, f1_score: 0.52693087  - 0.011001 \n",
            "Threshold 0.5900, f1_score: 0.51547888  - 0.011452 \n",
            "Threshold 0.6000, f1_score: 0.50309211  - 0.012387 \n",
            "Threshold 0.6100, f1_score: 0.49205293  - 0.011039 \n",
            "Threshold 0.6200, f1_score: 0.47969887  - 0.012354 \n",
            "Threshold 0.6300, f1_score: 0.46589834  - 0.013801 \n",
            "Threshold 0.6400, f1_score: 0.44897424  - 0.016924 \n",
            "Threshold 0.6500, f1_score: 0.43234103  - 0.016633 \n",
            "Threshold 0.6600, f1_score: 0.41587833  - 0.016463 \n",
            "Threshold 0.6700, f1_score: 0.39887718  - 0.017001 \n",
            "Threshold 0.6800, f1_score: 0.38167868  - 0.017199 \n",
            "Threshold 0.6900, f1_score: 0.36267505  - 0.019004 \n",
            "Threshold 0.7000, f1_score: 0.34249164  - 0.020183 \n",
            "Threshold 0.7100, f1_score: 0.32229733  - 0.020194 \n",
            "Threshold 0.7200, f1_score: 0.30221598  - 0.020081 \n",
            "Threshold 0.7300, f1_score: 0.27882749  - 0.023388 \n",
            "Threshold 0.7400, f1_score: 0.25596507  - 0.022862 \n",
            "Threshold 0.7500, f1_score: 0.23231681  - 0.023648 \n",
            "Threshold 0.7600, f1_score: 0.20774041  - 0.024576 \n",
            "Threshold 0.7700, f1_score: 0.18228709  - 0.025453 \n",
            "Threshold 0.7800, f1_score: 0.15731340  - 0.024974 \n",
            "Threshold 0.7900, f1_score: 0.13546854  - 0.021845 \n",
            "Threshold 0.8000, f1_score: 0.11236859  - 0.023100 \n",
            "Threshold 0.8100, f1_score: 0.08996096  - 0.022408 \n",
            "Threshold 0.8200, f1_score: 0.07164742  - 0.018314 \n",
            "Threshold 0.8300, f1_score: 0.05416667  - 0.017481 \n",
            "Threshold 0.8400, f1_score: 0.03892126  - 0.015245 \n",
            "Threshold 0.8500, f1_score: 0.02566431  - 0.013257 \n",
            "Threshold 0.8600, f1_score: 0.01642436  - 0.009240 \n",
            "Threshold 0.8700, f1_score: 0.00964885  - 0.006776 \n",
            "Threshold 0.8800, f1_score: 0.00563035  - 0.004019 \n",
            "Threshold 0.8900, f1_score: 0.00183959  - 0.003791 \n",
            "Threshold 0.9000, f1_score: 0.00061361  - 0.001226 \n",
            "Threshold 0.9100, f1_score: 0.00036823  - 0.000245 \n",
            "Threshold 0.9200, f1_score: 0.00024550  - 0.000123 \n",
            "Threshold 0.9300, f1_score: 0.00000000  - 0.000246 \n",
            "Threshold 0.9400, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9500, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9600, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9700, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9800, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9900, f1_score: 0.00000000  + 0.000000 \n",
            "Best f1 score 0.64173848312194, best threshold 0.33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAp-h_Ep8E6",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression \n",
        "Besides NB, we also tried LR to maximum `f1_score`\n",
        "\n",
        "2020.05.09: \n",
        "* Set `threshold = 0.21`, we got public score 0.60627 + private score = 0.62161. Bot are the maximum score we've got by now.\n",
        "<img class=\"center\" src=\"https://i.loli.net/2020/05/09/eE4KqloW52FDSdc.png\" alt=\"LR max f1_score\" width=850>\n",
        "* Set `threshold=0.25`, `public_score=0.60619` decreased, but `private_score=0.62166` increased. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4iCMLKZr68V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = C_LR()\n",
        "c.load_data()\n",
        "# b.train = b.train.sample(100000)\n",
        "c.countvectorize()\n",
        "labels = c.train.target\n",
        "x_train, x_val, y_train, y_val = train_test_split(c.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "\n",
        "model = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n",
        "model.fit(x_train, y_train)\n",
        "y_preds = model.predict(x_val)\n",
        "\n",
        "print(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "print(f\"Confusion matrix: \") \n",
        "print(confusion_matrix(y_val, y_preds))\n",
        "print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBA9t30Hs4jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the best threshold to maximum f1_score\n",
        "Helper().locate_threshold(model, x_val, y_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b421RTCRhymn",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "Since in this contest submissions are evaluated on **F1 Score** between the predicted and the observed targets. Our ultimate goal is to maximum the `f1_socre`. \n",
        "\n",
        "2020.05.09: \n",
        "* Naive Bayes achieved the maximum score `f1_score = 0.56456695` when the `threshold` is set to `0.726`. This result in a public score = 0.56452 + private score = 0.56706. The public socre is the best we've got with NB, but the private score is only second to 0.56889, one when we set the `threshold` to 0.6 .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wchbncDhALOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = C_Bayes()\n",
        "b.load_data()\n",
        "b.countvectorize()\n",
        "b.build_model()\n",
        "labels = b.train.target\n",
        "x_train, x_val, y_train, y_val = train_test_split(b.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "b.model.fit(x_train, y_train)\n",
        "y_preds = b.model.predict(x_val)\n",
        "\n",
        "logging.info(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "logging.info(f\"Confusion matrix: \") \n",
        "print(confusion_matrix(y_val, y_preds))\n",
        "print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_qcaj8UAsC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the best threshold to maximum f1_score\n",
        "Helper().locate_threshold(b.model, x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}