{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_Insincere_Questions_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPSd/jmwWWAcL9SJqPBBsBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoangLiu/ipynb/blob/master/Quora_Insincere_Questions_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3E1uAtsT17O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load packages \n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import timeit\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import logging\n",
        "import time\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "logging.basicConfig(format='[%(asctime)s %(levelname)8s] %(message)s', level=logging.INFO, datefmt='%m-%d %H:%M:%S')\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Dense, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_AjiQTkGDGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data\n",
        "! rm *.csv\n",
        "! wget -O quora.zip bwg.140714.xyz:8000/quora.zip \n",
        "! unzip quora.zip \n",
        "! ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bcd0v4GZiM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base class for classifier\n",
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    self.train = None\n",
        "    self.test = None \n",
        "    self.model = None\n",
        "\n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv(train_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      self.test = pd.read_csv(test_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      logging.info('CSV data loaded')\n",
        "  \n",
        "  def countvectorize(self):\n",
        "      tv = TfidfVectorizer(ngram_range=(1,3), token_pattern=r'\\w{1,}',\n",
        "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1, max_features=5000)\n",
        "      tv = CountVectorizer()\n",
        "      tv.fit(self.train.question_text)\n",
        "      self.vector_train = tv.transform(self.train.question_text)\n",
        "      self.vector_test  = tv.transform(self.test.question_text)\n",
        "      logging.info(\"Train & test text tokenized\")\n",
        "\n",
        "  def build_model(self):\n",
        "      pass\n",
        "\n",
        "  def run_model(self):\n",
        "      # Choose your own classifier: self.model and run it\n",
        "      logging.info(f\"{self.__class__.__name__} starts running.\")\n",
        "      labels = self.train.target\n",
        "      x_train, x_val, y_train, y_val = train_test_split(self.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "      self.model.fit(x_train, y_train)\n",
        "      y_preds = self.model.predict(x_val)\n",
        "\n",
        "      logging.info(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "      logging.info(f\"Confusion matrix: \") \n",
        "      print(confusion_matrix(y_val, y_preds))\n",
        "      print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))\n",
        "      # y_preds = self.model.predict(self.vector_test)\n",
        "      return y_preds\n",
        "\n",
        "  def save_predictions(self, y_preds):\n",
        "      sub = pd.read_csv(f\"sample_submission.csv\")\n",
        "      sub['prediction'] = y_preds \n",
        "      sub.to_csv(f\"submission_{self.__class__.__name__}.csv\", index=False)\n",
        "      logging.info('Prediction exported to submisison.csv')\n",
        "  \n",
        "  def pipeline(self):\n",
        "      s_time = time.clock()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "      self.build_model()\n",
        "      self.save_predictions(self.run_model())\n",
        "      logging.info(f\"Program running for {time.clock() - s_time} seconds\")\n",
        "\n",
        "class C_Bayes(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = MultinomialNB()\n",
        "      return self.model\n",
        "\n",
        "# Logistic Regression \n",
        "class C_LR(Classifier):\n",
        "  def build_model(self):\n",
        "      self.model = LogisticRegression(n_jobs=10, solver='lbfgs', C=0.1, verbose=1)\n",
        "      return self.model\n",
        "\n",
        "class C_SVM(Classifier):\n",
        "  def load_data(self, train_file='train.csv', test_file='test.csv'):\n",
        "      \"\"\" Load train, test csv files and return pandas.DataFrame\n",
        "      \"\"\"\n",
        "      self.train = pd.read_csv(train_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      self.train = self.train.sample(100000)\n",
        "      self.test = pd.read_csv(test_file, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "      logging.info('CSV data loaded')\n",
        "\n",
        "  def build_model(self):\n",
        "      self.model = svm.SVC()\n",
        "      return self.model\n",
        "\n",
        "class C_Ensemble(Classifier):\n",
        "  def ensemble(self):\n",
        "      s_time = time.perf_counter()\n",
        "      self.load_data()\n",
        "      self.countvectorize()\n",
        "\n",
        "      nb = MultinomialNB()\n",
        "      lr = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n",
        "      svc = svm.SVC()\n",
        "\n",
        "      all_preds = [0] * self.test.shape[0]\n",
        "      for m in (nb, lr, svc):\n",
        "          self.model = m\n",
        "          if m == svc: \n",
        "              self.load_data()\n",
        "              self.train = self.train.sample(10000)\n",
        "              self.countvectorize()\n",
        "          all_preds += self.run_model()\n",
        "\n",
        "      all_preds = [1 if p > 0 else 0 for p in all_preds]\n",
        "      self.save_predictions(all_preds)\n",
        "      logging.info(f\"Program running for {time.perf_counter() - s_time} seconds\")\n",
        "\n",
        "\n",
        "class Helper():\n",
        "    def locate_threshold(self, model, x_val, y_val):\n",
        "        y_probs = model.predict(x_val, batch_size=1024, verbose=1)\n",
        "        best_threshold = best_f1 = pre_f1 = 0\n",
        "        history = []\n",
        "\n",
        "        for i in np.arange(0.01, 1, 0.01):\n",
        "          if len(y_probs[0]) >= 2:\n",
        "              y2_preds = [1 if e[1] >= i else 0 for e in y_probs]\n",
        "          else:\n",
        "              y2_preds = (y_probs > i).astype(int)\n",
        "\n",
        "          cur_f1 = f1_score(y_val, y2_preds)\n",
        "          history.append((i, cur_f1))\n",
        "          symbol = '+' if cur_f1 >= pre_f1 else '-'\n",
        "          print(\"Threshold {:6.4f}, f1_score: {:<0.8f}  {} {:<0.6f} \".format(i, cur_f1, symbol, abs(cur_f1 - pre_f1)))\n",
        "          pre_f1 = cur_f1\n",
        "\n",
        "          if cur_f1 >= best_f1:\n",
        "              best_f1 = cur_f1\n",
        "              best_threshold = i\n",
        "\n",
        "        print(f\"Best f1 score {best_f1}, best threshold {best_threshold}\")\n",
        "        plt.xlabel('Threshold')\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.plot(*zip(*history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1mxAIjmVG_U",
        "colab_type": "text"
      },
      "source": [
        "## CNN\n",
        "We've tried linear models, the best result of private`f1_score` we got is 0.62166. Now we try Neural Network\n",
        "\n",
        "2020.05.11 \n",
        "\n",
        "* Biderectional GRU achieved `private_score` 0.64255, `public_score` 0.63140\n",
        "\n",
        "Seems that, smaller `batch_size` produces better performance, both on `precision` and `f1_score` .\n",
        "\n",
        "2020.05.12 Keras API GRU. \n",
        "\n",
        "* YES !!!! Finally reach `private_score=0.65443` and `public_score=0.63905` with Bidrectional GRU `max_features=50000, embed_size=300, max_len=100`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAe8YZhVVFKL",
        "colab_type": "code",
        "outputId": "e523c3d2-a608-42e1-ef32-960bc580e420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "class C_NN(Classifier):\n",
        "    def __init__(self, max_features=100000, embed_size=128, max_len=300):\n",
        "        self.max_features=max_features\n",
        "        self.embed_size=embed_size\n",
        "        self.max_len=max_len\n",
        "    \n",
        "    def tokenize_text(self, text_train, text_test):\n",
        "        '''@para: max_features, the most commenly used words in data set\n",
        "        @input are vector of text\n",
        "        '''\n",
        "        tokenizer = Tokenizer(num_words=self.max_features)\n",
        "        text = pd.concat([text_train, text_test])\n",
        "        tokenizer.fit_on_texts(text)\n",
        "\n",
        "        sequence_train = tokenizer.texts_to_sequences(text_train)\n",
        "        tokenized_train = pad_sequences(sequence_train, maxlen=self.max_len)\n",
        "        logging.info('Train text tokeninzed')\n",
        "\n",
        "        sequence_test = tokenizer.texts_to_sequences(text_test)\n",
        "        tokenized_test = pad_sequences(sequence_test, maxlen=self.max_len)\n",
        "        logging.info('Test text tokeninzed')\n",
        "        return tokenized_train, tokenized_test, tokenizer\n",
        "      \n",
        "    def build_model(self):\n",
        "        dropout = 0.2\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.max_features, self.embed_size, input_length=self.max_len))\n",
        "        model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model = model\n",
        "\n",
        "        return self.model\n",
        "        \n",
        "    def embed_word_vector(self, word_index, model='glove-wiki-gigaword-100'):\n",
        "        glove = api.load(model) # default: wikipedia 6B tokens, uncased\n",
        "        zeros = [0] * self.embed_size\n",
        "        matrix = np.zeros((self.max_features, self.embed_size))\n",
        "          \n",
        "        for word, i in word_index.items(): \n",
        "            if i >= self.max_features or word not in glove: continue # matrix[0] is zeros, that's also why >= is here\n",
        "            matrix[i] = glove[word]\n",
        "\n",
        "        logging.info('Matrix with embedded word vector created')\n",
        "        return matrix\n",
        "\n",
        "    def run(self, x_train, y_train):\n",
        "        checkpoint = ModelCheckpoint('weights_base_best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
        "\n",
        "        self.model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=2020)\n",
        "        BATCH_SIZE = max(16, 2 ** int(math.log(len(X_tra) / 100, 2)))\n",
        "        logging.info(f\"Batch size is set to {BATCH_SIZE}\")\n",
        "        history = self.model.fit(X_tra, y_tra, epochs=2, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), \\\n",
        "                              callbacks=[checkpoint, early], verbose=1)\n",
        "\n",
        "        y_pred = self.model.predict(X_val, batch_size=64, verbose=1)\n",
        "        y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "        print(classification_report(y_val, y_pred_bool))\n",
        "        return history\n",
        "\n",
        "class C_GRU():\n",
        "    def __init__(self):\n",
        "        self.embed_size=300\n",
        "        self.max_features=50000\n",
        "        self.max_len=200\n",
        "    \n",
        "    def build_model(self, embed_matrix=[]):\n",
        "        text_input = Input(shape=(self.max_len, ))\n",
        "        embed_text = layers.Embedding(self.max_features, self.embed_size)(text_input)\n",
        "        if len(embed_matrix) > 0:\n",
        "            embed_text = layers.Embedding(self.max_features, self.embed_size, \\\n",
        "                                          weights=[embed_matrix], trainable=False)(text_input)\n",
        "            \n",
        "\n",
        "        branch_a = layers.Bidirectional(layers.GRU(64, return_sequences=True))(embed_text)\n",
        "        branch_b = layers.GlobalMaxPool1D()(branch_a)\n",
        "        branch_c = layers.Dense(64, activation='relu')(branch_b)\n",
        "        branch_d = layers.Dropout(0.1)(branch_c)\n",
        "        branch_z = layers.Dense(1, activation='sigmoid')(branch_d)\n",
        "        \n",
        "        model = Model(inputs=text_input, outputs=branch_z)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        \n",
        "        return model\n",
        "\n",
        "c = C_NN(max_features=50000, embed_size=300, max_len=200)\n",
        "c.load_data()\n",
        "# # c.train = c.train[:300_000]\n",
        "vector_train, vector_test, tokenizer = c.tokenize_text(c.train.question_text, c.test.question_text)\n",
        "matrix = c.embed_word_vector(tokenizer.word_index, model='word2vec-google-news-300')\n",
        "model = C_GRU().build_model(matrix)\n",
        "\n",
        "# model.get_weights()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[05-13 02:20:39     INFO] CSV data loaded\n",
            "[05-13 02:21:34     INFO] Train text tokeninzed\n",
            "[05-13 02:21:42     INFO] Test text tokeninzed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 98.9% 1644.0/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[05-13 02:24:45     INFO] word2vec-google-news-300 downloaded\n",
            "[05-13 02:24:45     INFO] loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "[05-13 02:26:21     INFO] loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "[05-13 02:26:21     INFO] Matrix with embedded word vector created\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR6_t3qnej2I",
        "colab_type": "code",
        "outputId": "469acaa4-6d89-4f2c-9c84-804d97151592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(vector_train, c.train.target, train_size=0.8, random_state=2020)\n",
        "history = model.fit(X_tra, y_tra, epochs=1, batch_size=256, validation_data=(X_val, y_val))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1044897 samples, validate on 261225 samples\n",
            "Epoch 1/1\n",
            "1044897/1044897 [==============================] - 2807s 3ms/step - loss: 0.1197 - acc: 0.9539 - val_loss: 0.1088 - val_acc: 0.9569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg1HsYkOsWMv",
        "colab_type": "code",
        "outputId": "04c00b5e-7cb0-4332-bdfa-dd5c551fb199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Find maximum threshold \n",
        "Helper().locate_threshold(model, X_val, y_val)\n",
        "# 300_000 train size, batch_size = 1024, f1_score = 0.6118\n",
        "# 300_000 train size, batch_size = 1024, double GRU, f1_score = 0.6113\n",
        "# 300_000 train size, batch_size = 1024, with glove-wiki-gigaword-100 embeding, f1_score = 0.6030\n",
        "# 300_000 train size, batch_size = 1024, with google-news 300 embeding, f1_score = 0.6078\n",
        "# full train, bsize 1024, embed_size=300, maxlen=200, google news 300, f1_score = 0.6404\n",
        "# full train, bsize 1024, embed_size=300, maxlen=200, no embed, f1_score = 0.6390\n",
        "\n",
        "# Keras API full train, Bidirectional GRU 64, bsize 256, embed_size 300, maxlen 200, no embed, f1_score 0.6485\n",
        "# Keras API full train, Bidirectional GRU 64, bsize 256, embed_size 300, maxlen 200, fasttext 300, f1_score 0.6417\n",
        "# Keras API full train, Bidirectional GRU 64, bsize 256, embed_size 300, maxlen 200, google news 300, f1_score 0.6456\n",
        "\n",
        "# FYI, pretrained word vectors are not always winning"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "261225/261225 [==============================] - 17s 66us/step\n",
            "Threshold 0.0100, f1_score: 0.30196804  + 0.301968 \n",
            "Threshold 0.0200, f1_score: 0.37718462  + 0.075217 \n",
            "Threshold 0.0300, f1_score: 0.42460487  + 0.047420 \n",
            "Threshold 0.0400, f1_score: 0.45828618  + 0.033681 \n",
            "Threshold 0.0500, f1_score: 0.48417174  + 0.025886 \n",
            "Threshold 0.0600, f1_score: 0.50518665  + 0.021015 \n",
            "Threshold 0.0700, f1_score: 0.52308516  + 0.017899 \n",
            "Threshold 0.0800, f1_score: 0.53743841  + 0.014353 \n",
            "Threshold 0.0900, f1_score: 0.55049877  + 0.013060 \n",
            "Threshold 0.1000, f1_score: 0.56138344  + 0.010885 \n",
            "Threshold 0.1100, f1_score: 0.57100616  + 0.009623 \n",
            "Threshold 0.1200, f1_score: 0.57996602  + 0.008960 \n",
            "Threshold 0.1300, f1_score: 0.58795589  + 0.007990 \n",
            "Threshold 0.1400, f1_score: 0.59426371  + 0.006308 \n",
            "Threshold 0.1500, f1_score: 0.59944496  + 0.005181 \n",
            "Threshold 0.1600, f1_score: 0.60555069  + 0.006106 \n",
            "Threshold 0.1700, f1_score: 0.61132162  + 0.005771 \n",
            "Threshold 0.1800, f1_score: 0.61607702  + 0.004755 \n",
            "Threshold 0.1900, f1_score: 0.61938010  + 0.003303 \n",
            "Threshold 0.2000, f1_score: 0.62226693  + 0.002887 \n",
            "Threshold 0.2100, f1_score: 0.62532802  + 0.003061 \n",
            "Threshold 0.2200, f1_score: 0.62886700  + 0.003539 \n",
            "Threshold 0.2300, f1_score: 0.63073583  + 0.001869 \n",
            "Threshold 0.2400, f1_score: 0.63328446  + 0.002549 \n",
            "Threshold 0.2500, f1_score: 0.63557867  + 0.002294 \n",
            "Threshold 0.2600, f1_score: 0.63725999  + 0.001681 \n",
            "Threshold 0.2700, f1_score: 0.63889473  + 0.001635 \n",
            "Threshold 0.2800, f1_score: 0.64093772  + 0.002043 \n",
            "Threshold 0.2900, f1_score: 0.64155005  + 0.000612 \n",
            "Threshold 0.3000, f1_score: 0.64285325  + 0.001303 \n",
            "Threshold 0.3100, f1_score: 0.64359194  + 0.000739 \n",
            "Threshold 0.3200, f1_score: 0.64464654  + 0.001055 \n",
            "Threshold 0.3300, f1_score: 0.64464628  - 0.000000 \n",
            "Threshold 0.3400, f1_score: 0.64489819  + 0.000252 \n",
            "Threshold 0.3500, f1_score: 0.64491396  + 0.000016 \n",
            "Threshold 0.3600, f1_score: 0.64560135  + 0.000687 \n",
            "Threshold 0.3700, f1_score: 0.64476241  - 0.000839 \n",
            "Threshold 0.3800, f1_score: 0.64472549  - 0.000037 \n",
            "Threshold 0.3900, f1_score: 0.64398315  - 0.000742 \n",
            "Threshold 0.4000, f1_score: 0.64302874  - 0.000954 \n",
            "Threshold 0.4100, f1_score: 0.64186989  - 0.001159 \n",
            "Threshold 0.4200, f1_score: 0.64025583  - 0.001614 \n",
            "Threshold 0.4300, f1_score: 0.63893855  - 0.001317 \n",
            "Threshold 0.4400, f1_score: 0.63712438  - 0.001814 \n",
            "Threshold 0.4500, f1_score: 0.63518406  - 0.001940 \n",
            "Threshold 0.4600, f1_score: 0.63182821  - 0.003356 \n",
            "Threshold 0.4700, f1_score: 0.62918841  - 0.002640 \n",
            "Threshold 0.4800, f1_score: 0.62452247  - 0.004666 \n",
            "Threshold 0.4900, f1_score: 0.62144009  - 0.003082 \n",
            "Threshold 0.5000, f1_score: 0.61791653  - 0.003524 \n",
            "Threshold 0.5100, f1_score: 0.61360357  - 0.004313 \n",
            "Threshold 0.5200, f1_score: 0.60981463  - 0.003789 \n",
            "Threshold 0.5300, f1_score: 0.60439212  - 0.005423 \n",
            "Threshold 0.5400, f1_score: 0.59969320  - 0.004699 \n",
            "Threshold 0.5500, f1_score: 0.59281221  - 0.006881 \n",
            "Threshold 0.5600, f1_score: 0.58592004  - 0.006892 \n",
            "Threshold 0.5700, f1_score: 0.57809828  - 0.007822 \n",
            "Threshold 0.5800, f1_score: 0.57115943  - 0.006939 \n",
            "Threshold 0.5900, f1_score: 0.56405558  - 0.007104 \n",
            "Threshold 0.6000, f1_score: 0.55527590  - 0.008780 \n",
            "Threshold 0.6100, f1_score: 0.54357401  - 0.011702 \n",
            "Threshold 0.6200, f1_score: 0.53353549  - 0.010039 \n",
            "Threshold 0.6300, f1_score: 0.52048466  - 0.013051 \n",
            "Threshold 0.6400, f1_score: 0.50766947  - 0.012815 \n",
            "Threshold 0.6500, f1_score: 0.49372876  - 0.013941 \n",
            "Threshold 0.6600, f1_score: 0.48041931  - 0.013309 \n",
            "Threshold 0.6700, f1_score: 0.46420203  - 0.016217 \n",
            "Threshold 0.6800, f1_score: 0.44852552  - 0.015677 \n",
            "Threshold 0.6900, f1_score: 0.42836560  - 0.020160 \n",
            "Threshold 0.7000, f1_score: 0.41163803  - 0.016728 \n",
            "Threshold 0.7100, f1_score: 0.39072724  - 0.020911 \n",
            "Threshold 0.7200, f1_score: 0.36985518  - 0.020872 \n",
            "Threshold 0.7300, f1_score: 0.34743745  - 0.022418 \n",
            "Threshold 0.7400, f1_score: 0.32242713  - 0.025010 \n",
            "Threshold 0.7500, f1_score: 0.29586877  - 0.026558 \n",
            "Threshold 0.7600, f1_score: 0.26945520  - 0.026414 \n",
            "Threshold 0.7700, f1_score: 0.24202815  - 0.027427 \n",
            "Threshold 0.7800, f1_score: 0.21357135  - 0.028457 \n",
            "Threshold 0.7900, f1_score: 0.18549803  - 0.028073 \n",
            "Threshold 0.8000, f1_score: 0.15820029  - 0.027298 \n",
            "Threshold 0.8100, f1_score: 0.12971068  - 0.028490 \n",
            "Threshold 0.8200, f1_score: 0.10419916  - 0.025512 \n",
            "Threshold 0.8300, f1_score: 0.08041261  - 0.023787 \n",
            "Threshold 0.8400, f1_score: 0.05887939  - 0.021533 \n",
            "Threshold 0.8500, f1_score: 0.03997359  - 0.018906 \n",
            "Threshold 0.8600, f1_score: 0.02661344  - 0.013360 \n",
            "Threshold 0.8700, f1_score: 0.01727074  - 0.009343 \n",
            "Threshold 0.8800, f1_score: 0.00904203  - 0.008229 \n",
            "Threshold 0.8900, f1_score: 0.00453209  - 0.004510 \n",
            "Threshold 0.9000, f1_score: 0.00134953  - 0.003183 \n",
            "Threshold 0.9100, f1_score: 0.00036823  - 0.000981 \n",
            "Threshold 0.9200, f1_score: 0.00000000  - 0.000368 \n",
            "Threshold 0.9300, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9400, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9500, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9600, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9700, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9800, f1_score: 0.00000000  + 0.000000 \n",
            "Threshold 0.9900, f1_score: 0.00000000  + 0.000000 \n",
            "Best f1 score 0.6456013525330846, best threshold 0.36000000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAp-h_Ep8E6",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression \n",
        "Besides NB, we also tried LR to maximum `f1_score`\n",
        "\n",
        "2020.05.09: \n",
        "* Set `threshold = 0.21`, we got public score 0.60627 + private score = 0.62161. Bot are the maximum score we've got by now.\n",
        "<img class=\"center\" src=\"https://i.loli.net/2020/05/09/eE4KqloW52FDSdc.png\" alt=\"LR max f1_score\" width=850>\n",
        "* Set `threshold=0.25`, `public_score=0.60619` decreased, but `private_score=0.62166` increased. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4iCMLKZr68V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = C_LR()\n",
        "c.load_data()\n",
        "# b.train = b.train.sample(100000)\n",
        "c.countvectorize()\n",
        "labels = c.train.target\n",
        "x_train, x_val, y_train, y_val = train_test_split(c.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "\n",
        "model = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n",
        "model.fit(x_train, y_train)\n",
        "y_preds = model.predict(x_val)\n",
        "\n",
        "print(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "print(f\"Confusion matrix: \") \n",
        "print(confusion_matrix(y_val, y_preds))\n",
        "print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBA9t30Hs4jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the best threshold to maximum f1_score\n",
        "Helper().locate_threshold(model, x_val, y_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b421RTCRhymn",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "Since in this contest submissions are evaluated on **F1 Score** between the predicted and the observed targets. Our ultimate goal is to maximum the `f1_socre`. \n",
        "\n",
        "2020.05.09: \n",
        "* Naive Bayes achieved the maximum score `f1_score = 0.56456695` when the `threshold` is set to `0.726`. This result in a public score = 0.56452 + private score = 0.56706. The public socre is the best we've got with NB, but the private score is only second to 0.56889, one when we set the `threshold` to 0.6 .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wchbncDhALOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = C_Bayes()\n",
        "b.load_data()\n",
        "b.countvectorize()\n",
        "b.build_model()\n",
        "labels = b.train.target\n",
        "x_train, x_val, y_train, y_val = train_test_split(b.vector_train, labels, test_size=0.2, random_state=2090)\n",
        "b.model.fit(x_train, y_train)\n",
        "y_preds = b.model.predict(x_val)\n",
        "\n",
        "logging.info(f\"Accuracy score: {accuracy_score(y_val, y_preds)}\")\n",
        "logging.info(f\"Confusion matrix: \") \n",
        "print(confusion_matrix(y_val, y_preds))\n",
        "print(\"Classificaiton report:\\n\", classification_report(y_val, y_preds, target_names=[\"Sincere\", \"Insincere\"]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_qcaj8UAsC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the best threshold to maximum f1_score\n",
        "Helper().locate_threshold(b.model, x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}